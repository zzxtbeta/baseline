sbgnn.py --dataset_name bitcoin_alpha-1
#!/usr/bin/env python3
#-*- coding: utf-8 -*-
"""
@author: huangjunjie
@file: sbgnn.py
@time: 2021/03/28
"""


import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
import sys
import time
import random
import argparse
import subprocess

from collections import defaultdict

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score


from tqdm import tqdm

import logging
# https://docs.python.org/3/howto/logging.html#logging-advanced-tutorial


BASE_DIR = os.path.dirname(os.path.abspath(__file__))

parser = argparse.ArgumentParser()
parser.add_argument('--dirpath', default=BASE_DIR, help='Current Dir')
parser.add_argument('--device', type=str, default='cuda:1', help='Devices')
parser.add_argument('--dataset_name', type=str, default='bitcoin_alpha-1')
parser.add_argument('--a_emb_size', type=int, default=32, help='Embeding A Size')
parser.add_argument('--b_emb_size', type=int, default=32, help='Embeding B Size')
parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight Decay')
parser.add_argument('--lr', type=float, default=0.005, help='Learning Rate')
parser.add_argument('--seed', type=int, default=2023, help='Random seed')
parser.add_argument('--epoch', type=int, default=2000, help='Epoch')
parser.add_argument('--gnn_layer_num', type=int, default=2, help='GNN Layer')
parser.add_argument('--batch_size', type=int, default=500, help='Batch Size')
parser.add_argument('--dropout', type=float, default=0.5, help='Dropout')
parser.add_argument('--agg', type=str, default='AttentionAggregator', choices=['AttentionAggregator', 'MeanAggregator'], help='Aggregator')
args = parser.parse_args()


# TODO

exclude_hyper_params = ['dirpath', 'device']
hyper_params = dict(vars(args))
for exclude_p in exclude_hyper_params:
    del hyper_params[exclude_p]

hyper_params = "~".join([f"{k}-{v}" for k,v in hyper_params.items()])

from torch.utils.tensorboard import SummaryWriter
# https://pytorch.org/docs/stable/tensorboard.html
tb_writer = SummaryWriter(comment=hyper_params)


def setup_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
# setup seed
setup_seed(args.seed)

from common import DATA_EMB_DIC

# args.device = 'cpu'
args.device = torch.device(args.device)

class MeanAggregator(nn.Module):
    def __init__(self, a_dim, b_dim):
        super(MeanAggregator, self).__init__()

        self.out_mlp_layer = nn.Sequential(
            nn.Linear(b_dim, b_dim)
        )

    def forward(self, edge_dic_list: dict, feature_a, feature_b, node_num_a, node_num_b):

        edges = []
        for node in range(node_num_a):
            neighs = np.array(edge_dic_list[node]).reshape(-1, 1)
            a = np.array([node]).repeat(len(neighs)).reshape(-1, 1)
            edges.append(np.concatenate([a, neighs], axis=1))

        edges = np.vstack(edges)
        edges = torch.LongTensor(edges).to(args.device)
        matrix = torch.sparse_coo_tensor(edges.t(), torch.ones(edges.shape[0]), torch.Size([node_num_a, node_num_b]), device=args.device)
        row_sum = torch.spmm(matrix, torch.ones(size=(node_num_b, 1)).to(args.device))
        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(args.device), row_sum)

        new_emb = feature_b
        new_emb = self.out_mlp_layer(new_emb)
        output_emb = torch.spmm(matrix, new_emb)
        output_emb = output_emb.div(row_sum)

        return output_emb


class AttentionAggregator(nn.Module):
    def __init__(self, a_dim, b_dim):
        super(AttentionAggregator, self).__init__()

        self.out_mlp_layer = nn.Sequential(
            nn.Linear(b_dim, b_dim),
        )

        self.a = nn.Parameter(torch.FloatTensor(a_dim + b_dim, 1))
        nn.init.kaiming_normal_(self.a.data)

    def forward(self, edge_dic_list: dict, feature_a, feature_b, node_num_a, node_num_b):

        edges = []
        for node in range(node_num_a):
            neighs = np.array(edge_dic_list[node]).reshape(-1, 1)
            a = np.array([node]).repeat(len(neighs)).reshape(-1, 1)
            edges.append(np.concatenate([a, neighs], axis=1))

        edges = np.vstack(edges)
        edges = torch.LongTensor(edges).to(args.device)

        new_emb = feature_b
        new_emb = self.out_mlp_layer(new_emb)

        # print("=======139=====================================139================================139==============")
        # print(f"edge.shape:{edges.shape}")
        # print("Edges[:, 0]:", edges[:, 0].shape)
        # print("Edges[:, 1]:", edges[:, 1].shape)
        # print("===================================================================================================")
        # print("Max index in Edges[:, 0]:", torch.max(edges[:, 0]))
        # print("Min index in Edges[:, 0]:", torch.min(edges[:, 0]))
        edge_h_2 = torch.cat([feature_a[edges[:, 0]], new_emb[edges[:, 1]] ], dim=1)
        edges_h = torch.exp(F.elu(torch.einsum("ij,jl->il", [edge_h_2, self.a]), 0.1))

        matrix = torch.sparse_coo_tensor(edges.t(), edges_h[:, 0], torch.Size([node_num_a, node_num_b]), device=args.device)
        row_sum = torch.sparse.mm(matrix, torch.ones(size=(node_num_b, 1)).to(args.device))
        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(args.device), row_sum)

        output_emb = torch.sparse.mm(matrix, new_emb)
        output_emb = output_emb.div(row_sum)
        return output_emb



class SBGNNLayer(nn.Module):
    def __init__(self, edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg, \
        dataset_name=args.dataset_name, emb_size_a=32, emb_size_b=32, aggregator=MeanAggregator):
        super(SBGNNLayer, self).__init__()
        #
        self.set_a_num, self.set_b_num = DATA_EMB_DIC[dataset_name]

        # self.feature_a = feature_a
        # self.feature_b = feature_b
        self.edgelist_a_b_pos, self.edgelist_a_b_neg, self.edgelist_b_a_pos, self.edgelist_b_a_neg = \
            edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg
        self.edgelist_a_a_pos, self.edgelist_a_a_neg, self.edgelist_b_b_pos, self.edgelist_b_b_neg = \
            edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg

        self.agg_a_from_b_pos = aggregator(emb_size_b, emb_size_a)
        self.agg_a_from_b_neg = aggregator(emb_size_b, emb_size_a)
        self.agg_a_from_a_pos = aggregator(emb_size_a, emb_size_a)
        self.agg_a_from_a_neg = aggregator(emb_size_a, emb_size_a)

        self.agg_b_from_a_pos = aggregator(emb_size_a, emb_size_b)
        self.agg_b_from_a_neg = aggregator(emb_size_a, emb_size_b)
        self.agg_b_from_b_pos = aggregator(emb_size_b, emb_size_b)
        self.agg_b_from_b_neg = aggregator(emb_size_b, emb_size_b)

        self.update_func = nn.Sequential(
            nn.Dropout(args.dropout),
            nn.Linear(emb_size_a * 5, emb_size_a * 2),
            nn.PReLU(),
            nn.Linear(emb_size_b * 2, emb_size_b)

        )



    def forward(self, feature_a, feature_b):
        # assert feature_a.size()[0] == self.set_a_num, 'set_b_num error'
        # assert feature_b.size()[0] == self.set_b_num, 'set_b_num error'

        node_num_a, node_num_b = self.set_a_num, self.set_b_num

        m_a_from_b_pos = self.agg_a_from_b_pos(self.edgelist_a_b_pos, feature_a, feature_b, node_num_a, node_num_b)
        m_a_from_b_neg = self.agg_a_from_b_neg(self.edgelist_a_b_neg, feature_a, feature_b, node_num_a, node_num_b)
        m_a_from_a_pos = self.agg_a_from_a_pos(self.edgelist_a_a_pos, feature_a, feature_a, node_num_a, node_num_a)
        m_a_from_a_neg = self.agg_a_from_a_neg(self.edgelist_a_a_neg, feature_a, feature_a, node_num_a, node_num_a)

        new_feature_a = torch.cat([feature_a, m_a_from_b_pos, m_a_from_b_neg, m_a_from_a_pos, m_a_from_a_neg], dim=1)
        new_feature_a = self.update_func(new_feature_a)

        m_b_from_a_pos = self.agg_b_from_a_pos(self.edgelist_b_a_pos, feature_b, feature_a, node_num_b, node_num_a)
        m_b_from_a_neg = self.agg_b_from_a_neg(self.edgelist_b_a_neg, feature_b, feature_a, node_num_b, node_num_a)
        m_b_from_b_pos = self.agg_b_from_b_pos(self.edgelist_b_b_pos, feature_b, feature_b, node_num_b, node_num_b)
        m_b_from_b_neg = self.agg_b_from_b_neg(self.edgelist_b_b_neg, feature_b, feature_b, node_num_b, node_num_b)

        new_feature_b = torch.cat([feature_b, m_b_from_a_pos, m_b_from_a_neg, m_b_from_b_pos, m_b_from_b_neg], dim=1)
        new_feature_b = self.update_func(new_feature_b)

        return new_feature_a, new_feature_b



class SBGNN(nn.Module):
    def __init__(self, edgelists,
                    dataset_name=args.dataset_name, layer_num=1, emb_size_a=32, emb_size_b=32, aggregator=AttentionAggregator):
        super(SBGNN, self).__init__()

        # assert edgelists must compelte
        assert len(edgelists) == 8, 'must 8 edgelists'
        edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg = edgelists

        self.set_a_num, self.set_b_num = DATA_EMB_DIC[dataset_name]

        self.features_a = nn.Embedding(self.set_a_num, emb_size_a)
        self.features_b = nn.Embedding(self.set_b_num, emb_size_b)
        self.features_a.weight.requires_grad = True
        self.features_b.weight.requires_grad = True
        # features_a = features_a.to(args.device)
        # features_b = features_b.to(args.device)

        self.layers = nn.ModuleList(
            [SBGNNLayer(edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg, \
                    dataset_name=dataset_name, emb_size_a=32, emb_size_b=32, aggregator=aggregator) for _ in range(layer_num)]
        )
        # self.mlp = nn.Sequential(
        #     nn.Linear(emb_size_a * 3, 30),
        #     nn.PReLU(),
        #     nn.Linear(30, 1),
        #     nn.Sigmoid()
        # )
        # def init_weights(m):
        #     if type(m) == nn.Linear:
        #         torch.nn.init.xavier_uniform_(m.weight)
        #         m.bias.data.fill_(0.01)
        # self.apply(init_weights)


    def get_embeddings(self):
        emb_a = self.features_a(torch.arange(self.set_a_num).to(args.device))
        emb_b = self.features_b(torch.arange(self.set_b_num).to(args.device))
        for m in self.layers:
            emb_a, emb_b = m(emb_a, emb_b)
        return emb_a, emb_b

    def forward(self, edge_lists):
        embedding_a, embedding_b = self.get_embeddings()

        #### with mlp
        # emb_concat = torch.cat([embedding_a[edge_lists[:, 0]], embedding_b[edge_lists[:, 1]], embedding_a[edge_lists[:, 0]] * embedding_b[edge_lists[:, 1]] ], dim=1)
        # y = self.mlp(emb_concat).squeeze(-1)
        # return y

        ## without mlp
        y = torch.einsum("ij, ij->i", [embedding_a[edge_lists[:, 0]] , embedding_b[edge_lists[:, 1]] ])
        y = torch.sigmoid(y)  # 添加sigmoid激活函数

        # Check for NaN in the output
        if torch.isnan(y).any():
            print(f"此时的y值：{y}")
            print("NaN values found in the forward output y!")

        return y

    def loss(self, pred_y, y):
        assert y.min() >= 0, 'must 0~1'
        assert pred_y.size() == y.size(), 'must be same length'
        pos_ratio = y.sum() /  y.size()[0]
        weight = torch.where(y > 0.5, 1./pos_ratio, 1./(1-pos_ratio))

        # Check for NaN in the loss
        if torch.isnan(weight).any():
            print("NaN values found in the loss weights!")

        # weight = torch.where(y > 0.5, (1-pos_ratio), pos_ratio)
        return F.binary_cross_entropy_with_logits(pred_y, y, weight=weight)


# =========== function
def load_data(dataset_name):
    train_file_path = os.path.join('topic_data/bitcoin_datasets', f'{dataset_name}_training.txt')
    val_file_path = os.path.join('topic_data/bitcoin_datasets', f'{dataset_name}_validation.txt')
    test_file_path = os.path.join('topic_data/bitcoin_datasets', f'{dataset_name}_testing.txt')

    train_edgelist = []
    with open(train_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s ,k= map(int, line.split('\t'))
            train_edgelist.append((a, b, s, k))

    val_edgelist = []
    with open(val_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s, k = map(int, line.split('\t'))
            val_edgelist.append((a, b, s, k))

    test_edgelist = []
    with open(test_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s, k = map(int, line.split('\t'))
            test_edgelist.append((a, b, s, k))

    # print(f"np.array(val_edgelist): {np.array(val_edgelist)}")
    return np.array(train_edgelist), np.array(val_edgelist), np.array(test_edgelist)


# ============= load data
def load_edgelists(edge_lists):
    edgelist_a_b_pos, edgelist_a_b_neg = defaultdict(list), defaultdict(list)
    edgelist_b_a_pos, edgelist_b_a_neg = defaultdict(list), defaultdict(list)
    edgelist_a_a_pos, edgelist_a_a_neg = defaultdict(list), defaultdict(list)
    edgelist_b_b_pos, edgelist_b_b_neg = defaultdict(list), defaultdict(list)

    for a, b, s ,k in edge_lists:
        if s == 1:
            edgelist_a_b_pos[a].append(b)
            edgelist_b_a_pos[b].append(a)
        elif s== -1:
            edgelist_a_b_neg[a].append(b)
            edgelist_b_a_neg[b].append(a)
        else:
            print(a, b, s, k)
            raise Exception("s must be -1/1")

    edge_list_a_a = defaultdict(lambda: defaultdict(int))
    edge_list_b_b = defaultdict(lambda: defaultdict(int))
    for a, b, s, k in edge_lists:
        for b2 in edgelist_a_b_pos[a]:
            edge_list_b_b[b][b2] += 1 * s
        for b2 in edgelist_a_b_neg[a]:
            edge_list_b_b[b][b2] -= 1 * s
        for a2 in edgelist_b_a_pos[b]:
            edge_list_a_a[a][a2] += 1 * s
        for a2 in edgelist_b_a_neg[b]:
            edge_list_a_a[a][a2] -= 1 * s

    for a1 in edge_list_a_a:
        for a2 in edge_list_a_a[a1]:
            v = edge_list_a_a[a1][a2]
            if a1 == a2: continue
            if v > 0:
                edgelist_a_a_pos[a1].append(a2)
            elif v < 0:
                edgelist_a_a_neg[a1].append(a2)

    for b1 in edge_list_b_b:
        for b2 in edge_list_b_b[b1]:
            v = edge_list_b_b[b1][b2]
            if b1 == b2: continue
            if v > 0:
                edgelist_b_b_pos[b1].append(b2)
            elif v < 0:
                edgelist_b_b_neg[b1].append(b2)

    return edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg

def find_best_threshold(pred_y, y):
    best_threshold = 0.5
    best_f1 = 0

    preds = pred_y.cpu().numpy() if isinstance(pred_y, torch.Tensor) else pred_y
    y = y.cpu().numpy() if isinstance(y, torch.Tensor) else y

    for threshold in np.arange(0.01, 1, 0.01):
        thresholded_preds = (preds >= threshold).astype(int)

        f1 = f1_score(y, thresholded_preds)

        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold

    return best_threshold, best_f1



@torch.no_grad()
def test_and_val(pred_y, y, mode='val', epoch=0):
    preds = pred_y.cpu().numpy()
    y = y.cpu().numpy()

    # preds[preds >= 0.5]  = 1
    # preds[preds < 0.5] = 0
    # test_y = y

    # Find the best threshold
    best_threshold, best_f1 = find_best_threshold(pred_y, y)
    print(f"best_threshold: {best_threshold} ， best_f1: {best_f1}")
    # Apply the best threshold
    preds[preds >= best_threshold] = 1
    preds[preds < best_threshold] = 0
    test_y = y

    auc = roc_auc_score(test_y, preds)
    precision = precision_score(test_y, preds)
    recall = recall_score(test_y,preds)
    f1 = f1_score(test_y, preds)
    macro_f1 = f1_score(test_y, preds, average='macro')
    micro_f1 = f1_score(test_y, preds, average='micro')
    pos_ratio = np.sum(test_y) /  len(test_y)
    res = {
        f'{mode}_auc': auc,
        f'{mode}_precision': precision,
        f'{mode}_recall': recall,
        f'{mode}_f1' : f1,
        f'{mode}_pos_ratio': pos_ratio,
        f'{mode}_epoch': epoch,
        f'{mode}_macro_f1' : macro_f1,
        f'{mode}_micro_f1' : micro_f1,
    }
    for k, v in res.items():
        mode ,_, metric = k.partition('_')
        tb_writer.add_scalar(f'{metric}/{mode}', v, epoch)
    # tb_writer.add_scalar( f'{mode}_auc', auc, epoch)
    # tb_writer.add_scalar( f'{mode}_f1', auc, epoch)
    return res



def run():
    train_edgelist, val_edgelist, test_edgelist  = load_data(args.dataset_name)

    set_a_num, set_b_num = DATA_EMB_DIC[args.dataset_name]
    train_y = np.array([i[-1] for i in train_edgelist])
    val_y   = np.array([i[-1] for i in val_edgelist])
    test_y  = np.array([i[-1] for i in test_edgelist])

    # train_y = torch.from_numpy( (train_y + 1)/2 ).float().to(args.device)
    # val_y = torch.from_numpy( (val_y + 1)/2 ).float().to(args.device)
    # test_y = torch.from_numpy( (test_y + 1)/2 ).float().to(args.device)

    train_y = torch.from_numpy( train_y ).float().to(args.device)
    val_y = torch.from_numpy( val_y).float().to(args.device)
    test_y = torch.from_numpy( test_y).float().to(args.device)
    # get edge lists
    edgelists = load_edgelists(train_edgelist)

    if args.agg == 'MeanAggregator':
        agg = MeanAggregator
    else:
        agg = AttentionAggregator

    model = SBGNN(edgelists, dataset_name=args.dataset_name, layer_num=args.gnn_layer_num, aggregator=agg)
    model = model.to(args.device)

    print(model.train())
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    res_best = {'val_auc': 0}
    for epoch in tqdm(range(1, args.epoch + 2)):
        # train
        model.train()
        optimizer.zero_grad()
        pred_y = model(train_edgelist)
        loss = model.loss(pred_y, train_y)
        loss.backward()
        optimizer.step()
        print('loss', loss)


        res_cur = {}
        # if epoch % 5 == 0:
        if True:
        # val/test
            model.eval()
            pred_y = model(train_edgelist)
            # print("test_y 中的唯一值:", np.unique(test_y.cpu().detach().numpy()))
            # print("preds 中的唯一值:", np.unique(pred_y.cpu().detach().numpy()))
            if torch.isnan(pred_y).any():
                print("pred_y中出现了Nan")
                print(f"此时的pred_y值：{[pred_y]}")

            res = test_and_val(pred_y, train_y, mode='train', epoch=epoch)
            res_cur.update(res)
            pred_val_y = model(val_edgelist)
            res = test_and_val(pred_val_y, val_y, mode='val', epoch=epoch)
            res_cur.update(res)
            pred_test_y = model(test_edgelist)
            res = test_and_val(pred_test_y, test_y, mode='test', epoch=epoch)
            res_cur.update(res)
            if res_cur['val_auc'] > res_best['val_auc']:
                res_best = res_cur
                print(res_best)
    print('Done! Best Results:')
    print(res_best)
    print_list = ['test_auc', 'test_f1', 'test_macro_f1', 'test_micro_f1']
    for i in print_list:
        print(i, res_best[i], end=' ')



def main():
    print(" ".join(sys.argv))
    this_fpath = os.path.abspath(__file__)
    t = subprocess.run(f'cat {this_fpath}', shell=True, stdout=subprocess.PIPE)
    print(str(t.stdout, 'utf-8'))
    print('=' * 20)
    run()

if __name__ == "__main__":
    main()

====================
SBGNN(
  (features_a): Embedding(3286, 32)
  (features_b): Embedding(3754, 32)
  (layers): ModuleList(
    (0-1): 2 x SBGNNLayer(
      (agg_a_from_b_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_b_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_a_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_a_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_a_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_a_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_b_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_b_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (update_func): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Linear(in_features=160, out_features=64, bias=True)
        (2): PReLU(num_parameters=1)
        (3): Linear(in_features=64, out_features=32, bias=True)
      )
    )
  )
)
  0%|          | 0/2001 [00:00<?, ?it/s]  0%|          | 1/2001 [00:13<7:34:12, 13.63s/it]  0%|          | 2/2001 [00:25<6:55:43, 12.48s/it]  0%|          | 3/2001 [00:35<6:27:09, 11.63s/it]  0%|          | 4/2001 [00:46<6:15:39, 11.29s/it]  0%|          | 5/2001 [00:58<6:23:19, 11.52s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 6/2001 [01:11<6:33:57, 11.85s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 7/2001 [01:25<6:56:14, 12.52s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 8/2001 [01:39<7:18:16, 13.19s/it]  0%|          | 9/2001 [01:55<7:44:23, 13.99s/it]  0%|          | 10/2001 [02:11<8:08:12, 14.71s/it]  1%|          | 11/2001 [02:27<8:15:35, 14.94s/it]  1%|          | 12/2001 [02:43<8:25:30, 15.25s/it]  1%|          | 13/2001 [02:58<8:24:47, 15.24s/it]  1%|          | 14/2001 [03:12<8:13:26, 14.90s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 15/2001 [03:26<8:06:42, 14.70s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 16/2001 [03:42<8:14:04, 14.93s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 17/2001 [03:57<8:18:30, 15.08s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 18/2001 [04:13<8:28:58, 15.40s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 19/2001 [04:27<8:16:06, 15.02s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 20/2001 [04:42<8:08:55, 14.81s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 21/2001 [04:57<8:14:56, 15.00s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 22/2001 [05:12<8:15:08, 15.01s/it]/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  1%|          | 23/2001 [05:27<8:13:33, 14.97s/it]loss tensor(1.4591, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0.03755784177186756
best_threshold: 0.5 ， best_f1: 0.03692307692307692
best_threshold: 0.51 ， best_f1: 0.039334341906202726
{'train_auc': 0.502554219105559, 'train_precision': 0.019138317842589625, 'train_recall': 1.0, 'train_f1': 0.03755784177186756, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 1, 'train_macro_f1': 0.023861395589071053, 'train_micro_f1': 0.024053573869072024, 'val_auc': 0.5003990422984836, 'val_precision': 0.018808777429467086, 'val_recall': 1.0, 'val_f1': 0.03692307692307692, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 1, 'val_macro_f1': 0.019258986627407677, 'val_micro_f1': 0.01957713390759593, 'test_auc': 0.5125147347740668, 'test_precision': 0.02127659574468085, 'test_recall': 0.26, 'test_f1': 0.039334341906202726, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 1, 'test_macro_f1': 0.44956339528518346, 'test_micro_f1': 0.7552986512524085}
loss tensor(1.4498, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.46 ， best_f1: 0.039032304242019374
best_threshold: 0.46 ， best_f1: 0.038929440389294405
best_threshold: 0.46 ， best_f1: 0.04027386226339105
{'train_auc': 0.5218123534434851, 'train_precision': 0.019913335605433565, 'train_recall': 0.9784688995215312, 'train_f1': 0.039032304242019374, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 2, 'train_macro_f1': 0.08066237134088329, 'train_micro_f1': 0.08254749214158809, 'val_auc': 0.5271348762968875, 'val_precision': 0.019851116625310174, 'val_recall': 1.0, 'val_f1': 0.038929440389294405, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 2, 'val_macro_f1': 0.0709408746231105, 'val_micro_f1': 0.07204385277995301, 'test_auc': 0.5318271119842829, 'test_precision': 0.02055076037813399, 'test_recall': 1.0, 'test_f1': 0.04027386226339105, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 2, 'test_macro_f1': 0.07998177782545246, 'test_micro_f1': 0.08169556840077072}
loss tensor(1.4413, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.31 ， best_f1: 0.041142974401067046
best_threshold: 0.31 ， best_f1: 0.0414866032843561
best_threshold: 0.31 ， best_f1: 0.04304778303917348
{'train_auc': 0.5460515483411796, 'train_precision': 0.021022280471821756, 'train_recall': 0.9593301435406698, 'train_f1': 0.041142974401067046, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 3, 'train_macro_f1': 0.1377004150550134, 'train_micro_f1': 0.14851259623707347, 'val_auc': 0.5574620909816441, 'val_precision': 0.02118270079435128, 'val_recall': 1.0, 'val_f1': 0.0414866032843561, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 3, 'val_macro_f1': 0.12382132598004492, 'val_micro_f1': 0.13155833985904464, 'test_auc': 0.5632612966601178, 'test_precision': 0.02199736031676199, 'test_recall': 1.0, 'test_f1': 0.04304778303917348, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 3, 'test_macro_f1': 0.13383641331937746, 'test_micro_f1': 0.14335260115606938}
loss tensor(1.4226, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.03 ， best_f1: 0.04139422334345879
best_threshold: 0.03 ， best_f1: 0.0415944540727903
best_threshold: 0.03 ， best_f1: 0.04306632213608958
{'train_auc': 0.5488963529217533, 'train_precision': 0.021152328334648775, 'train_recall': 0.9617224880382775, 'train_f1': 0.04139422334345879, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 4, 'train_macro_f1': 0.14039154363381937, 'train_micro_f1': 0.15179262903740148, 'val_auc': 0.558659217877095, 'val_precision': 0.021238938053097345, 'val_recall': 1.0, 'val_f1': 0.0415944540727903, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 4, 'val_macro_f1': 0.12579722703639515, 'val_micro_f1': 0.13390759592795615, 'test_auc': 0.5634577603143418, 'test_precision': 0.022007042253521125, 'test_recall': 1.0, 'test_f1': 0.04306632213608958, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 4, 'test_macro_f1': 0.13415519733024842, 'test_micro_f1': 0.14373795761078997}
loss tensor(1.3968, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.01 ， best_f1: 0.010226442658875092
best_threshold: 0.01 ， best_f1: 0.03883495145631068
best_threshold: 0.01 ， best_f1: 0.024509803921568627
loss tensor(1.3877, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.29000000000000004 ， best_f1: 0.012903225806451613
best_threshold: 0.33 ， best_f1: 0.020905923344947737
loss tensor(1.3867, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.14 ， best_f1: 0.01282051282051282
best_threshold: 0.2 ， best_f1: 0.02112676056338028
loss tensor(1.3865, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.04 ， best_f1: 0.01282051282051282
best_threshold: 0.08 ， best_f1: 0.02120141342756184
loss tensor(1.3865, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.01 ， best_f1: 0.02027027027027027
loss tensor(1.3864, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.060000000000000005 ， best_f1: 0.008658008658008658
loss tensor(1.3864, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.02 ， best_f1: 0.008658008658008658
loss tensor(1.3864, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3864, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3864, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3864, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
loss tensor(1.3863, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
best_threshold: 0.5 ， best_f1: 0
  1%|          | 23/2001 [05:36<8:01:42, 14.61s/it]
此时的y值：tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
NaN values found in the forward output y!
loss tensor(nan, device='cuda:1', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
此时的y值：tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
NaN values found in the forward output y!
pred_y中出现了Nan
此时的pred_y值：[tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
best_threshold: 0.5 ， best_f1: 0
Traceback (most recent call last):
  File "sbgnn.py", line 523, in <module>
    main()
  File "sbgnn.py", line 520, in main
    run()
  File "sbgnn.py", line 495, in run
    res = test_and_val(pred_y, train_y, mode='train', epoch=epoch)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "sbgnn.py", line 416, in test_and_val
    auc = roc_auc_score(test_y, preds)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/_param_validation.py", line 214, in wrapper
    return func(*args, **kwargs)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_ranking.py", line 606, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/validation.py", line 957, in check_array
    _assert_all_finite(
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/validation.py", line 122, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/validation.py", line 171, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.
