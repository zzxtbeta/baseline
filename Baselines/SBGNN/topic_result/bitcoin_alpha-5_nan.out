sbgnn.py --dataset_name bitcoin_alpha-5
#!/usr/bin/env python3
#-*- coding: utf-8 -*-
"""
@author: huangjunjie
@file: sbgnn.py
@time: 2021/03/28
"""


import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
import sys
import time
import random
import argparse
import subprocess

from collections import defaultdict

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score


from tqdm import tqdm

import logging
# https://docs.python.org/3/howto/logging.html#logging-advanced-tutorial


BASE_DIR = os.path.dirname(os.path.abspath(__file__))

parser = argparse.ArgumentParser()
parser.add_argument('--dirpath', default=BASE_DIR, help='Current Dir')
parser.add_argument('--device', type=str, default='cuda:1', help='Devices')
parser.add_argument('--dataset_name', type=str, default='bitcoin_alpha-1')
parser.add_argument('--a_emb_size', type=int, default=32, help='Embeding A Size')
parser.add_argument('--b_emb_size', type=int, default=32, help='Embeding B Size')
parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight Decay')
parser.add_argument('--lr', type=float, default=0.005, help='Learning Rate')
parser.add_argument('--seed', type=int, default=2023, help='Random seed')
parser.add_argument('--epoch', type=int, default=2000, help='Epoch')
parser.add_argument('--gnn_layer_num', type=int, default=2, help='GNN Layer')
parser.add_argument('--batch_size', type=int, default=500, help='Batch Size')
parser.add_argument('--dropout', type=float, default=0.5, help='Dropout')
parser.add_argument('--agg', type=str, default='AttentionAggregator', choices=['AttentionAggregator', 'MeanAggregator'], help='Aggregator')
args = parser.parse_args()


# TODO

exclude_hyper_params = ['dirpath', 'device']
hyper_params = dict(vars(args))
for exclude_p in exclude_hyper_params:
    del hyper_params[exclude_p]

hyper_params = "~".join([f"{k}-{v}" for k,v in hyper_params.items()])

from torch.utils.tensorboard import SummaryWriter
# https://pytorch.org/docs/stable/tensorboard.html
tb_writer = SummaryWriter(comment=hyper_params)


def setup_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
# setup seed
setup_seed(args.seed)

from common import DATA_EMB_DIC

# args.device = 'cpu'
args.device = torch.device(args.device)

class MeanAggregator(nn.Module):
    def __init__(self, a_dim, b_dim):
        super(MeanAggregator, self).__init__()

        self.out_mlp_layer = nn.Sequential(
            nn.Linear(b_dim, b_dim)
        )

    def forward(self, edge_dic_list: dict, feature_a, feature_b, node_num_a, node_num_b):

        edges = []
        for node in range(node_num_a):
            neighs = np.array(edge_dic_list[node]).reshape(-1, 1)
            a = np.array([node]).repeat(len(neighs)).reshape(-1, 1)
            edges.append(np.concatenate([a, neighs], axis=1))

        edges = np.vstack(edges)
        edges = torch.LongTensor(edges).to(args.device)
        matrix = torch.sparse_coo_tensor(edges.t(), torch.ones(edges.shape[0]), torch.Size([node_num_a, node_num_b]), device=args.device)
        row_sum = torch.spmm(matrix, torch.ones(size=(node_num_b, 1)).to(args.device))
        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(args.device), row_sum)

        new_emb = feature_b
        new_emb = self.out_mlp_layer(new_emb)
        output_emb = torch.spmm(matrix, new_emb)
        output_emb = output_emb.div(row_sum)

        return output_emb


class AttentionAggregator(nn.Module):
    def __init__(self, a_dim, b_dim):
        super(AttentionAggregator, self).__init__()

        self.out_mlp_layer = nn.Sequential(
            nn.Linear(b_dim, b_dim),
        )

        self.a = nn.Parameter(torch.FloatTensor(a_dim + b_dim, 1))
        nn.init.kaiming_normal_(self.a.data)

    def forward(self, edge_dic_list: dict, feature_a, feature_b, node_num_a, node_num_b):

        edges = []
        for node in range(node_num_a):
            neighs = np.array(edge_dic_list[node]).reshape(-1, 1)
            a = np.array([node]).repeat(len(neighs)).reshape(-1, 1)
            edges.append(np.concatenate([a, neighs], axis=1))

        edges = np.vstack(edges)
        edges = torch.LongTensor(edges).to(args.device)

        new_emb = feature_b
        new_emb = self.out_mlp_layer(new_emb)

        # print("=======139=====================================139================================139==============")
        # print(f"edge.shape:{edges.shape}")
        # print("Edges[:, 0]:", edges[:, 0].shape)
        # print("Edges[:, 1]:", edges[:, 1].shape)
        # print("===================================================================================================")
        # print("Max index in Edges[:, 0]:", torch.max(edges[:, 0]))
        # print("Min index in Edges[:, 0]:", torch.min(edges[:, 0]))
        edge_h_2 = torch.cat([feature_a[edges[:, 0]], new_emb[edges[:, 1]] ], dim=1)
        edges_h = torch.exp(F.elu(torch.einsum("ij,jl->il", [edge_h_2, self.a]), 0.1))

        matrix = torch.sparse_coo_tensor(edges.t(), edges_h[:, 0], torch.Size([node_num_a, node_num_b]), device=args.device)
        row_sum = torch.sparse.mm(matrix, torch.ones(size=(node_num_b, 1)).to(args.device))
        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(args.device), row_sum)

        output_emb = torch.sparse.mm(matrix, new_emb)
        output_emb = output_emb.div(row_sum)
        return output_emb



class SBGNNLayer(nn.Module):
    def __init__(self, edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg, \
        dataset_name=args.dataset_name, emb_size_a=32, emb_size_b=32, aggregator=MeanAggregator):
        super(SBGNNLayer, self).__init__()
        #
        self.set_a_num, self.set_b_num = DATA_EMB_DIC[dataset_name]

        # self.feature_a = feature_a
        # self.feature_b = feature_b
        self.edgelist_a_b_pos, self.edgelist_a_b_neg, self.edgelist_b_a_pos, self.edgelist_b_a_neg = \
            edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg
        self.edgelist_a_a_pos, self.edgelist_a_a_neg, self.edgelist_b_b_pos, self.edgelist_b_b_neg = \
            edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg

        self.agg_a_from_b_pos = aggregator(emb_size_b, emb_size_a)
        self.agg_a_from_b_neg = aggregator(emb_size_b, emb_size_a)
        self.agg_a_from_a_pos = aggregator(emb_size_a, emb_size_a)
        self.agg_a_from_a_neg = aggregator(emb_size_a, emb_size_a)

        self.agg_b_from_a_pos = aggregator(emb_size_a, emb_size_b)
        self.agg_b_from_a_neg = aggregator(emb_size_a, emb_size_b)
        self.agg_b_from_b_pos = aggregator(emb_size_b, emb_size_b)
        self.agg_b_from_b_neg = aggregator(emb_size_b, emb_size_b)

        self.update_func = nn.Sequential(
            nn.Dropout(args.dropout),
            nn.Linear(emb_size_a * 5, emb_size_a * 2),
            nn.PReLU(),
            nn.Linear(emb_size_b * 2, emb_size_b)

        )



    def forward(self, feature_a, feature_b):
        # assert feature_a.size()[0] == self.set_a_num, 'set_b_num error'
        # assert feature_b.size()[0] == self.set_b_num, 'set_b_num error'

        node_num_a, node_num_b = self.set_a_num, self.set_b_num

        m_a_from_b_pos = self.agg_a_from_b_pos(self.edgelist_a_b_pos, feature_a, feature_b, node_num_a, node_num_b)
        m_a_from_b_neg = self.agg_a_from_b_neg(self.edgelist_a_b_neg, feature_a, feature_b, node_num_a, node_num_b)
        m_a_from_a_pos = self.agg_a_from_a_pos(self.edgelist_a_a_pos, feature_a, feature_a, node_num_a, node_num_a)
        m_a_from_a_neg = self.agg_a_from_a_neg(self.edgelist_a_a_neg, feature_a, feature_a, node_num_a, node_num_a)

        new_feature_a = torch.cat([feature_a, m_a_from_b_pos, m_a_from_b_neg, m_a_from_a_pos, m_a_from_a_neg], dim=1)
        new_feature_a = self.update_func(new_feature_a)

        m_b_from_a_pos = self.agg_b_from_a_pos(self.edgelist_b_a_pos, feature_b, feature_a, node_num_b, node_num_a)
        m_b_from_a_neg = self.agg_b_from_a_neg(self.edgelist_b_a_neg, feature_b, feature_a, node_num_b, node_num_a)
        m_b_from_b_pos = self.agg_b_from_b_pos(self.edgelist_b_b_pos, feature_b, feature_b, node_num_b, node_num_b)
        m_b_from_b_neg = self.agg_b_from_b_neg(self.edgelist_b_b_neg, feature_b, feature_b, node_num_b, node_num_b)

        new_feature_b = torch.cat([feature_b, m_b_from_a_pos, m_b_from_a_neg, m_b_from_b_pos, m_b_from_b_neg], dim=1)
        new_feature_b = self.update_func(new_feature_b)

        return new_feature_a, new_feature_b



class SBGNN(nn.Module):
    def __init__(self, edgelists,
                    dataset_name=args.dataset_name, layer_num=1, emb_size_a=32, emb_size_b=32, aggregator=AttentionAggregator):
        super(SBGNN, self).__init__()

        # assert edgelists must compelte
        assert len(edgelists) == 8, 'must 8 edgelists'
        edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg = edgelists

        self.set_a_num, self.set_b_num = DATA_EMB_DIC[dataset_name]

        self.features_a = nn.Embedding(self.set_a_num, emb_size_a)
        self.features_b = nn.Embedding(self.set_b_num, emb_size_b)
        self.features_a.weight.requires_grad = True
        self.features_b.weight.requires_grad = True
        # features_a = features_a.to(args.device)
        # features_b = features_b.to(args.device)

        self.layers = nn.ModuleList(
            [SBGNNLayer(edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg, \
                    dataset_name=dataset_name, emb_size_a=32, emb_size_b=32, aggregator=aggregator) for _ in range(layer_num)]
        )
        # self.mlp = nn.Sequential(
        #     nn.Linear(emb_size_a * 3, 30),
        #     nn.PReLU(),
        #     nn.Linear(30, 1),
        #     nn.Sigmoid()
        # )
        # def init_weights(m):
        #     if type(m) == nn.Linear:
        #         torch.nn.init.xavier_uniform_(m.weight)
        #         m.bias.data.fill_(0.01)
        # self.apply(init_weights)


    def get_embeddings(self):
        emb_a = self.features_a(torch.arange(self.set_a_num).to(args.device))
        emb_b = self.features_b(torch.arange(self.set_b_num).to(args.device))
        for m in self.layers:
            emb_a, emb_b = m(emb_a, emb_b)
        return emb_a, emb_b

    def forward(self, edge_lists):
        embedding_a, embedding_b = self.get_embeddings()

        #### with mlp
        # emb_concat = torch.cat([embedding_a[edge_lists[:, 0]], embedding_b[edge_lists[:, 1]], embedding_a[edge_lists[:, 0]] * embedding_b[edge_lists[:, 1]] ], dim=1)
        # y = self.mlp(emb_concat).squeeze(-1)
        # return y

        ## without mlp
        y = torch.einsum("ij, ij->i", [embedding_a[edge_lists[:, 0]] , embedding_b[edge_lists[:, 1]] ])
        y = torch.sigmoid(y)  # 添加sigmoid激活函数

        # Check for NaN in the output
        if torch.isnan(y).any():
            print(f"此时的y值：{y}")
            print("NaN values found in the forward output y!")

        return y

    def loss(self, pred_y, y):
        assert y.min() >= 0, 'must 0~1'
        assert pred_y.size() == y.size(), 'must be same length'
        pos_ratio = y.sum() /  y.size()[0]
        weight = torch.where(y > 0.5, 1./pos_ratio, 1./(1-pos_ratio))

        # Check for NaN in the loss
        if torch.isnan(weight).any():
            print("NaN values found in the loss weights!")

        # weight = torch.where(y > 0.5, (1-pos_ratio), pos_ratio)
        return F.binary_cross_entropy_with_logits(pred_y, y, weight=weight)


# =========== function
def load_data(dataset_name):
    train_file_path = os.path.join('topic_data/bitcoin_datasets', f'{dataset_name}_training.txt')
    val_file_path = os.path.join('topic_data/bitcoin_datasets', f'{dataset_name}_validation.txt')
    test_file_path = os.path.join('topic_data/bitcoin_datasets', f'{dataset_name}_testing.txt')

    train_edgelist = []
    with open(train_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s ,k= map(int, line.split('\t'))
            train_edgelist.append((a, b, s, k))

    val_edgelist = []
    with open(val_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s, k = map(int, line.split('\t'))
            val_edgelist.append((a, b, s, k))

    test_edgelist = []
    with open(test_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s, k = map(int, line.split('\t'))
            test_edgelist.append((a, b, s, k))

    # print(f"np.array(val_edgelist): {np.array(val_edgelist)}")
    return np.array(train_edgelist), np.array(val_edgelist), np.array(test_edgelist)


# ============= load data
def load_edgelists(edge_lists):
    edgelist_a_b_pos, edgelist_a_b_neg = defaultdict(list), defaultdict(list)
    edgelist_b_a_pos, edgelist_b_a_neg = defaultdict(list), defaultdict(list)
    edgelist_a_a_pos, edgelist_a_a_neg = defaultdict(list), defaultdict(list)
    edgelist_b_b_pos, edgelist_b_b_neg = defaultdict(list), defaultdict(list)

    for a, b, s ,k in edge_lists:
        if s == 1:
            edgelist_a_b_pos[a].append(b)
            edgelist_b_a_pos[b].append(a)
        elif s== -1:
            edgelist_a_b_neg[a].append(b)
            edgelist_b_a_neg[b].append(a)
        else:
            print(a, b, s, k)
            raise Exception("s must be -1/1")

    edge_list_a_a = defaultdict(lambda: defaultdict(int))
    edge_list_b_b = defaultdict(lambda: defaultdict(int))
    for a, b, s, k in edge_lists:
        for b2 in edgelist_a_b_pos[a]:
            edge_list_b_b[b][b2] += 1 * s
        for b2 in edgelist_a_b_neg[a]:
            edge_list_b_b[b][b2] -= 1 * s
        for a2 in edgelist_b_a_pos[b]:
            edge_list_a_a[a][a2] += 1 * s
        for a2 in edgelist_b_a_neg[b]:
            edge_list_a_a[a][a2] -= 1 * s

    for a1 in edge_list_a_a:
        for a2 in edge_list_a_a[a1]:
            v = edge_list_a_a[a1][a2]
            if a1 == a2: continue
            if v > 0:
                edgelist_a_a_pos[a1].append(a2)
            elif v < 0:
                edgelist_a_a_neg[a1].append(a2)

    for b1 in edge_list_b_b:
        for b2 in edge_list_b_b[b1]:
            v = edge_list_b_b[b1][b2]
            if b1 == b2: continue
            if v > 0:
                edgelist_b_b_pos[b1].append(b2)
            elif v < 0:
                edgelist_b_b_neg[b1].append(b2)

    return edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg

def find_best_threshold(pred_y, y):
    best_threshold = 0.5
    best_f1 = 0

    preds = pred_y.cpu().numpy() if isinstance(pred_y, torch.Tensor) else pred_y
    y = y.cpu().numpy() if isinstance(y, torch.Tensor) else y

    for threshold in np.arange(0.01, 1, 0.01):
        thresholded_preds = (preds >= threshold).astype(int)

        f1 = f1_score(y, thresholded_preds)

        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold

    return best_threshold, best_f1



@torch.no_grad()
def test_and_val(pred_y, y, mode='val', epoch=0):
    preds = pred_y.cpu().numpy()
    y = y.cpu().numpy()

    # preds[preds >= 0.5]  = 1
    # preds[preds < 0.5] = 0
    # test_y = y

    # Find the best threshold
    best_threshold, best_f1 = find_best_threshold(pred_y, y)
    print(f"best_threshold: {best_threshold} ， best_f1: {best_f1}")
    # Apply the best threshold
    preds[preds >= best_threshold] = 1
    preds[preds < best_threshold] = 0
    test_y = y

    auc = roc_auc_score(test_y, preds)
    precision = precision_score(test_y, preds)
    recall = recall_score(test_y,preds)
    f1 = f1_score(test_y, preds)
    macro_f1 = f1_score(test_y, preds, average='macro')
    micro_f1 = f1_score(test_y, preds, average='micro')
    pos_ratio = np.sum(test_y) /  len(test_y)
    res = {
        f'{mode}_auc': auc,
        f'{mode}_precision': precision,
        f'{mode}_recall': recall,
        f'{mode}_f1' : f1,
        f'{mode}_pos_ratio': pos_ratio,
        f'{mode}_epoch': epoch,
        f'{mode}_macro_f1' : macro_f1,
        f'{mode}_micro_f1' : micro_f1,
    }
    for k, v in res.items():
        mode ,_, metric = k.partition('_')
        tb_writer.add_scalar(f'{metric}/{mode}', v, epoch)
    # tb_writer.add_scalar( f'{mode}_auc', auc, epoch)
    # tb_writer.add_scalar( f'{mode}_f1', auc, epoch)
    return res



def run():
    train_edgelist, val_edgelist, test_edgelist  = load_data(args.dataset_name)

    set_a_num, set_b_num = DATA_EMB_DIC[args.dataset_name]
    train_y = np.array([i[-1] for i in train_edgelist])
    val_y   = np.array([i[-1] for i in val_edgelist])
    test_y  = np.array([i[-1] for i in test_edgelist])

    # train_y = torch.from_numpy( (train_y + 1)/2 ).float().to(args.device)
    # val_y = torch.from_numpy( (val_y + 1)/2 ).float().to(args.device)
    # test_y = torch.from_numpy( (test_y + 1)/2 ).float().to(args.device)

    train_y = torch.from_numpy( train_y ).float().to(args.device)
    val_y = torch.from_numpy( val_y).float().to(args.device)
    test_y = torch.from_numpy( test_y).float().to(args.device)
    # get edge lists
    edgelists = load_edgelists(train_edgelist)

    if args.agg == 'MeanAggregator':
        agg = MeanAggregator
    else:
        agg = AttentionAggregator

    model = SBGNN(edgelists, dataset_name=args.dataset_name, layer_num=args.gnn_layer_num, aggregator=agg)
    model = model.to(args.device)

    print(model.train())
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    res_best = {'val_auc': 0}
    for epoch in tqdm(range(1, args.epoch + 2)):
        # train
        model.train()
        optimizer.zero_grad()
        pred_y = model(train_edgelist)
        loss = model.loss(pred_y, train_y)
        loss.backward()
        optimizer.step()
        print('loss', loss)


        res_cur = {}
        # if epoch % 5 == 0:
        if True:
        # val/test
            model.eval()
            pred_y = model(train_edgelist)
            # print("test_y 中的唯一值:", np.unique(test_y.cpu().detach().numpy()))
            # print("preds 中的唯一值:", np.unique(pred_y.cpu().detach().numpy()))
            if torch.isnan(pred_y).any():
                print("pred_y中出现了Nan")
                print(f"此时的pred_y值：{[pred_y]}")

            res = test_and_val(pred_y, train_y, mode='train', epoch=epoch)
            res_cur.update(res)
            pred_val_y = model(val_edgelist)
            res = test_and_val(pred_val_y, val_y, mode='val', epoch=epoch)
            res_cur.update(res)
            pred_test_y = model(test_edgelist)
            res = test_and_val(pred_test_y, test_y, mode='test', epoch=epoch)
            res_cur.update(res)
            if res_cur['val_auc'] > res_best['val_auc']:
                res_best = res_cur
                print(res_best)
    print('Done! Best Results:')
    print(res_best)
    print_list = ['test_auc', 'test_f1', 'test_macro_f1', 'test_micro_f1']
    for i in print_list:
        print(i, res_best[i], end=' ')



def main():
    print(" ".join(sys.argv))
    this_fpath = os.path.abspath(__file__)
    t = subprocess.run(f'cat {this_fpath}', shell=True, stdout=subprocess.PIPE)
    print(str(t.stdout, 'utf-8'))
    print('=' * 20)
    run()

if __name__ == "__main__":
    main()

====================
SBGNN(
  (features_a): Embedding(3783, 32)
  (features_b): Embedding(3783, 32)
  (layers): ModuleList(
    (0-1): 2 x SBGNNLayer(
      (agg_a_from_b_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_b_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_a_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_a_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_a_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_a_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_b_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_b_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (update_func): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Linear(in_features=160, out_features=64, bias=True)
        (2): PReLU(num_parameters=1)
        (3): Linear(in_features=64, out_features=32, bias=True)
      )
    )
  )
)
  0%|          | 0/2001 [00:00<?, ?it/s]  0%|          | 1/2001 [00:21<11:59:08, 21.57s/it]  0%|          | 2/2001 [00:41<11:33:07, 20.80s/it]  0%|          | 3/2001 [01:00<10:55:18, 19.68s/it]  0%|          | 4/2001 [01:19<10:48:40, 19.49s/it]  0%|          | 5/2001 [01:37<10:26:33, 18.83s/it]  0%|          | 6/2001 [01:55<10:25:05, 18.80s/it]  0%|          | 7/2001 [02:13<10:10:04, 18.36s/it]  0%|          | 8/2001 [02:31<10:06:07, 18.25s/it]  0%|          | 9/2001 [02:47<9:49:46, 17.76s/it]   0%|          | 10/2001 [03:06<9:52:39, 17.86s/it]  1%|          | 11/2001 [03:24<9:58:32, 18.05s/it]  1%|          | 12/2001 [03:42<9:54:52, 17.94s/it]  1%|          | 13/2001 [03:59<9:47:35, 17.73s/it]loss tensor(1.4638, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.52 ， best_f1: 0.040097699979645844
best_threshold: 0.53 ， best_f1: 0.09523809523809525
best_threshold: 0.53 ， best_f1: 0.128
{'train_auc': 0.5338237937675276, 'train_precision': 0.020484558594156183, 'train_recall': 0.9425837320574163, 'train_f1': 0.040097699979645844, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 1, 'train_macro_f1': 0.1311003963815755, 'train_micro_f1': 0.14063140631406315, 'val_auc': 0.548134477254589, 'val_precision': 0.07692307692307693, 'val_recall': 0.125, 'val_f1': 0.09523809523809525, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 1, 'val_macro_f1': 0.5361778593412476, 'val_micro_f1': 0.9553641346906813, 'test_auc': 0.5668369351669941, 'test_precision': 0.10666666666666667, 'test_recall': 0.16, 'test_f1': 0.128, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 1, 'test_macro_f1': 0.5532398815399802, 'test_micro_f1': 0.9579961464354528}
loss tensor(1.4531, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.07868020304568528
best_threshold: 0.52 ， best_f1: 0.06896551724137931
best_threshold: 0.52 ， best_f1: 0.11382113821138211
loss tensor(1.4462, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.47000000000000003 ， best_f1: 0.05966438781852082
best_threshold: 0.5 ， best_f1: 0.07547169811320753
best_threshold: 0.51 ， best_f1: 0.0790960451977401
{'train_auc': 0.530875609418458, 'train_precision': 0.04030226700251889, 'train_recall': 0.11483253588516747, 'train_f1': 0.05966438781852082, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 3, 'train_macro_f1': 0.5119450731091281, 'train_micro_f1': 0.9310737551819963, 'val_auc': 0.5522080340516096, 'val_precision': 0.04878048780487805, 'val_recall': 0.16666666666666666, 'val_f1': 0.07547169811320753, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 3, 'val_macro_f1': 0.5177195091873227, 'val_micro_f1': 0.9232576350822239, 'test_auc': 0.5464243614931238, 'test_precision': 0.05511811023622047, 'test_recall': 0.14, 'test_f1': 0.0790960451977401, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 3, 'test_macro_f1': 0.5232902926966159, 'test_micro_f1': 0.9371868978805395}
loss tensor(1.4336, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.38 ， best_f1: 0.07348703170028818
best_threshold: 0.5 ， best_f1: 0.0808080808080808
best_threshold: 0.52 ， best_f1: 0.10434782608695653
{'train_auc': 0.5396654450707349, 'train_precision': 0.05257731958762887, 'train_recall': 0.12200956937799043, 'train_f1': 0.07348703170028818, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 4, 'train_macro_f1': 0.5216190862504828, 'train_micro_f1': 0.9414149697052526, 'val_auc': 0.5550013301409951, 'val_precision': 0.05333333333333334, 'val_recall': 0.16666666666666666, 'val_f1': 0.0808080808080808, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 4, 'val_macro_f1': 0.5218704355160567, 'val_micro_f1': 0.9287392325763508, 'test_auc': 0.548408644400786, 'test_precision': 0.09230769230769231, 'test_recall': 0.12, 'test_f1': 0.10434782608695653, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 4, 'test_macro_f1': 0.5420261297922467, 'test_micro_f1': 0.9603082851637764}
loss tensor(1.3770, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.16 ， best_f1: 0.0941385435168739
best_threshold: 0.47000000000000003 ， best_f1: 0.07339449541284404
best_threshold: 0.53 ， best_f1: 0.10810810810810811
loss tensor(1.3818, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.09999999999999999 ， best_f1: 0.17738791423001948
best_threshold: 0.01 ， best_f1: 0.07457627118644067
best_threshold: 0.02 ， best_f1: 0.10097087378640777
{'train_auc': 0.5968468448450214, 'train_precision': 0.14967105263157895, 'train_recall': 0.21770334928229665, 'train_f1': 0.17738791423001948, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 6, 'train_macro_f1': 0.5788516210762001, 'train_micro_f1': 0.9615507266183773, 'val_auc': 0.6254156690609204, 'val_precision': 0.04059040590405904, 'val_recall': 0.4583333333333333, 'val_f1': 0.07457627118644067, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 6, 'val_macro_f1': 0.47686316879375157, 'val_micro_f1': 0.7862176977290525, 'test_auc': 0.6737524557956779, 'test_precision': 0.05591397849462366, 'test_recall': 0.52, 'test_f1': 0.10097087378640777, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 6, 'test_macro_f1': 0.5009667203156638, 'test_micro_f1': 0.8215799614643545}
loss tensor(1.3641, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.45 ， best_f1: 0.37001897533206835
best_threshold: 0.62 ， best_f1: 0.17777777777777778
best_threshold: 0.81 ， best_f1: 0.28571428571428564
loss tensor(1.2411, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.98 ， best_f1: 0.3408
best_threshold: 0.98 ， best_f1: 0.3661971830985916
best_threshold: 0.98 ， best_f1: 0.3164556962025316
{'train_auc': 0.7404114014830248, 'train_precision': 0.25600961538461536, 'train_recall': 0.5095693779904307, 'train_f1': 0.3408, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 8, 'train_macro_f1': 0.6607404295226484, 'train_micro_f1': 0.9624618468406906, 'val_auc': 0.7572658951848895, 'val_precision': 0.2765957446808511, 'val_recall': 0.5416666666666666, 'val_f1': 0.3661971830985916, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 8, 'val_macro_f1': 0.6740369725400328, 'val_micro_f1': 0.9647611589663273, 'test_auc': 0.7336935166994107, 'test_precision': 0.23148148148148148, 'test_recall': 0.5, 'test_f1': 0.3164556962025316, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 8, 'test_macro_f1': 0.6474965285464168, 'test_micro_f1': 0.9583815028901734}
loss tensor(1.2566, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.86 ， best_f1: 0.39203084832904883
best_threshold: 0.89 ， best_f1: 0.35135135135135137
best_threshold: 0.92 ， best_f1: 0.3312101910828026
loss tensor(1.1756, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.9400000000000001 ， best_f1: 0.39802631578947373
best_threshold: 0.98 ， best_f1: 0.3389830508474576
best_threshold: 0.9600000000000001 ， best_f1: 0.33082706766917297
loss tensor(1.2190, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.97 ， best_f1: 0.4111204717775906
best_threshold: 0.99 ， best_f1: 0.33333333333333337
best_threshold: 0.99 ， best_f1: 0.3358778625954198
loss tensor(1.1402, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.4228094575799722
best_threshold: 0.99 ， best_f1: 0.36111111111111105
best_threshold: 0.99 ， best_f1: 0.32298136645962733
loss tensor(1.1262, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.4126597687157638
best_threshold: 0.99 ， best_f1: 0.37647058823529417
best_threshold: 0.97 ， best_f1: 0.29292929292929293
{'train_auc': 0.8849293184579049, 'train_precision': 0.276734693877551, 'train_recall': 0.8110047846889952, 'train_f1': 0.4126597687157638, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 13, 'train_macro_f1': 0.6949121981845223, 'train_micro_f1': 0.9560384492733817, 'val_auc': 0.8153764299015696, 'val_precision': 0.26229508196721313, 'val_recall': 0.6666666666666666, 'val_f1': 0.37647058823529417, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 13, 'val_macro_f1': 0.6775022037976796, 'val_micro_f1': 0.9584964761158966, 'test_auc': 0.7666208251473478, 'test_precision': 0.19594594594594594, 'test_recall': 0.58, 'test_f1': 0.29292929292929293, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 13, 'test_macro_f1': 0.6324422105672106, 'test_micro_f1': 0.9460500963391136}
loss tensor(1.1382, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
  1%|          | 14/2001 [04:17<9:46:54, 17.72s/it]  1%|          | 15/2001 [04:34<9:40:56, 17.55s/it]  1%|          | 16/2001 [04:53<9:54:16, 17.96s/it]  1%|          | 17/2001 [05:11<9:56:06, 18.03s/it]  1%|          | 18/2001 [05:29<10:00:51, 18.18s/it]  1%|          | 19/2001 [05:47<9:56:43, 18.06s/it]   1%|          | 20/2001 [06:06<10:00:56, 18.20s/it]  1%|          | 21/2001 [06:24<10:03:20, 18.28s/it]  1%|          | 22/2001 [06:42<9:59:12, 18.17s/it]   1%|          | 23/2001 [06:59<9:49:44, 17.89s/it]  1%|          | 24/2001 [07:18<9:54:31, 18.04s/it]  1%|          | 25/2001 [07:36<9:53:34, 18.02s/it]  1%|▏         | 26/2001 [07:53<9:49:10, 17.90s/it]  1%|▏         | 27/2001 [08:12<9:55:14, 18.09s/it]  1%|▏         | 28/2001 [08:30<9:58:30, 18.20s/it]  1%|▏         | 29/2001 [08:48<9:51:24, 17.99s/it]  1%|▏         | 30/2001 [09:05<9:44:46, 17.80s/it]  2%|▏         | 31/2001 [09:23<9:39:43, 17.66s/it]  2%|▏         | 32/2001 [09:41<9:49:50, 17.97s/it]  2%|▏         | 33/2001 [09:59<9:49:47, 17.98s/it]  2%|▏         | 34/2001 [10:18<9:52:17, 18.07s/it]  2%|▏         | 35/2001 [10:36<9:57:09, 18.22s/it]  2%|▏         | 36/2001 [10:55<10:06:16, 18.51s/it]best_threshold: 0.99 ， best_f1: 0.40889957907396274
best_threshold: 0.99 ， best_f1: 0.37209302325581395
best_threshold: 0.99 ， best_f1: 0.29292929292929293
loss tensor(1.1430, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.4133738601823708
best_threshold: 0.99 ， best_f1: 0.36363636363636365
best_threshold: 0.99 ， best_f1: 0.29145728643216084
loss tensor(1.0889, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.4268675455116133
best_threshold: 0.99 ， best_f1: 0.367816091954023
best_threshold: 0.99 ， best_f1: 0.29591836734693877
loss tensor(1.1075, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.42821158690176325
best_threshold: 0.99 ， best_f1: 0.3516483516483516
best_threshold: 0.99 ， best_f1: 0.29591836734693877
loss tensor(1.1110, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.04 ， best_f1: 0.41375076734192756
best_threshold: 0.99 ， best_f1: 0.28571428571428575
best_threshold: 0.99 ， best_f1: 0.28571428571428575
loss tensor(1.1046, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.4186770428015564
best_threshold: 0.99 ， best_f1: 0.2926829268292683
best_threshold: 0.99 ， best_f1: 0.2976190476190476
loss tensor(1.1621, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.9600000000000001 ， best_f1: 0.4232887490165224
best_threshold: 0.99 ， best_f1: 0.27906976744186046
best_threshold: 0.99 ， best_f1: 0.2793296089385475
loss tensor(1.1312, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.46 ， best_f1: 0.4210526315789474
best_threshold: 0.46 ， best_f1: 0.23880597014925373
best_threshold: 0.98 ， best_f1: 0.2558139534883721
loss tensor(1.3016, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.38705035971223023
best_threshold: 0.99 ， best_f1: 0.2448979591836735
best_threshold: 0.99 ， best_f1: 0.23696682464454977
loss tensor(1.1644, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.38724373576309795
best_threshold: 0.99 ， best_f1: 0.27350427350427353
best_threshold: 0.99 ， best_f1: 0.23293172690763053
loss tensor(1.1198, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.3595980962453728
best_threshold: 0.99 ， best_f1: 0.2741935483870968
best_threshold: 0.99 ， best_f1: 0.23484848484848483
{'train_auc': 0.8803901078060447, 'train_precision': 0.2308214528173795, 'train_recall': 0.8133971291866029, 'train_f1': 0.3595980962453728, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 24, 'train_macro_f1': 0.6653861562610311, 'train_micro_f1': 0.9448316705389276, 'val_auc': 0.8210461558925247, 'val_precision': 0.17, 'val_recall': 0.7083333333333334, 'val_f1': 0.2741935483870968, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 24, 'val_macro_f1': 0.6185782556750299, 'val_micro_f1': 0.9295223179326546, 'test_auc': 0.7740471512770137, 'test_precision': 0.14485981308411214, 'test_recall': 0.62, 'test_f1': 0.23484848484848483, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 24, 'test_macro_f1': 0.5969207913483188, 'test_micro_f1': 0.9221579961464355}
loss tensor(1.1248, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.32242769084874345
best_threshold: 0.99 ， best_f1: 0.2608695652173913
best_threshold: 0.99 ， best_f1: 0.21453287197231835
{'train_auc': 0.8753281099423007, 'train_precision': 0.20106445890005914, 'train_recall': 0.8133971291866029, 'train_f1': 0.32242769084874345, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 25, 'train_macro_f1': 0.6441176809949217, 'train_micro_f1': 0.9349004601157123, 'val_auc': 0.8366919393455706, 'val_precision': 0.15789473684210525, 'val_recall': 0.75, 'val_f1': 0.2608695652173913, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 25, 'val_macro_f1': 0.6093255110855169, 'val_micro_f1': 0.9201252936570086, 'test_auc': 0.7691355599214146, 'test_precision': 0.1297071129707113, 'test_recall': 0.62, 'test_f1': 0.21453287197231835, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 25, 'test_macro_f1': 0.5841078969125008, 'test_micro_f1': 0.9125240847784202}
loss tensor(1.1118, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.28169014084507044
best_threshold: 0.99 ， best_f1: 0.2337662337662338
best_threshold: 0.99 ， best_f1: 0.18934911242603553
loss tensor(1.0924, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.27803992740471867
best_threshold: 0.99 ， best_f1: 0.25287356321839083
best_threshold: 0.99 ， best_f1: 0.22404371584699453
{'train_auc': 0.912761751907665, 'train_precision': 0.16388532306375694, 'train_recall': 0.916267942583732, 'train_f1': 0.27803992740471867, 'train_pos_ratio': 0.019042412646348685, 'train_epoch': 27, 'train_macro_f1': 0.6148505224308206, 'train_micro_f1': 0.9093890938909389, 'val_auc': 0.9072559191274274, 'val_precision': 0.14666666666666667, 'val_recall': 0.9166666666666666, 'val_f1': 0.25287356321839083, 'val_pos_ratio': 0.018794048551292093, 'val_epoch': 27, 'val_macro_f1': 0.5991258572394476, 'val_micro_f1': 0.8981989036805011, 'test_auc': 0.8559724950884086, 'test_precision': 0.12974683544303797, 'test_recall': 0.82, 'test_f1': 0.22404371584699453, 'test_pos_ratio': 0.019267822736030827, 'test_epoch': 27, 'test_macro_f1': 0.5825857053530162, 'test_micro_f1': 0.8905587668593449}
loss tensor(1.1196, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.27783822996010155
best_threshold: 0.99 ， best_f1: 0.25142857142857145
best_threshold: 0.99 ， best_f1: 0.21983914209115282
loss tensor(1.1214, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.28006589785831965
best_threshold: 0.99 ， best_f1: 0.22641509433962267
best_threshold: 0.9600000000000001 ， best_f1: 0.18579234972677597
loss tensor(1.1225, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.3065825067628494
best_threshold: 0.98 ， best_f1: 0.24675324675324675
best_threshold: 0.98 ， best_f1: 0.20668693009118538
loss tensor(1.1092, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.3333333333333333
best_threshold: 0.99 ， best_f1: 0.2585034013605442
best_threshold: 0.99 ， best_f1: 0.22077922077922082
loss tensor(1.1340, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.3423967774420947
best_threshold: 0.99 ， best_f1: 0.2533333333333333
best_threshold: 0.99 ， best_f1: 0.2222222222222222
loss tensor(1.0917, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.38245219347581555
best_threshold: 0.9500000000000001 ， best_f1: 0.25503355704697983
best_threshold: 0.99 ， best_f1: 0.23611111111111108
loss tensor(1.1210, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.36253369272237196
best_threshold: 0.51 ， best_f1: 0.22535211267605634
best_threshold: 0.99 ， best_f1: 0.23437499999999997
loss tensor(1.1396, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.38210227272727265
best_threshold: 0.75 ， best_f1: 0.23357664233576644
best_threshold: 0.99 ， best_f1: 0.2439024390243903
loss tensor(1.1103, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.98 ， best_f1: 0.3944281524926686
best_threshold: 0.9400000000000001 ， best_f1: 0.23703703703703705
best_threshold: 0.99 ， best_f1: 0.24
loss   2%|▏         | 37/2001 [11:13<9:55:07, 18.18s/it]   2%|▏         | 38/2001 [11:32<10:01:06, 18.37s/it]  2%|▏         | 39/2001 [11:50<10:01:54, 18.41s/it]  2%|▏         | 40/2001 [12:08<9:55:50, 18.23s/it]   2%|▏         | 41/2001 [12:25<9:42:18, 17.83s/it]  2%|▏         | 42/2001 [12:42<9:37:16, 17.68s/it]  2%|▏         | 43/2001 [13:00<9:36:47, 17.68s/it]  2%|▏         | 44/2001 [13:17<9:29:15, 17.45s/it]  2%|▏         | 45/2001 [13:33<9:18:21, 17.13s/it]  2%|▏         | 46/2001 [13:49<9:10:18, 16.89s/it]  2%|▏         | 47/2001 [14:04<8:45:11, 16.13s/it]  2%|▏         | 48/2001 [14:19<8:40:17, 15.98s/it]  2%|▏         | 49/2001 [14:36<8:44:52, 16.13s/it]  2%|▏         | 50/2001 [14:52<8:43:31, 16.10s/it]  3%|▎         | 51/2001 [15:08<8:39:33, 15.99s/it]  3%|▎         | 52/2001 [15:24<8:44:09, 16.14s/it]  3%|▎         | 53/2001 [15:39<8:31:56, 15.77s/it]  3%|▎         | 54/2001 [15:54<8:26:25, 15.61s/it]  3%|▎         | 54/2001 [16:03<9:39:05, 17.85s/it]
tensor(1.1663, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.99 ， best_f1: 0.3985185185185185
best_threshold: 0.9400000000000001 ， best_f1: 0.2406015037593985
best_threshold: 0.99 ， best_f1: 0.23904382470119523
loss tensor(1.1987, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.68 ， best_f1: 0.3991097922848665
best_threshold: 0.78 ， best_f1: 0.2442748091603053
best_threshold: 0.98 ， best_f1: 0.23166023166023164
loss tensor(1.1671, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.67 ， best_f1: 0.3991097922848665
best_threshold: 0.8 ， best_f1: 0.2442748091603053
best_threshold: 0.99 ， best_f1: 0.22310756972111556
loss tensor(1.1916, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.92 ， best_f1: 0.3811074918566775
best_threshold: 0.8400000000000001 ， best_f1: 0.21311475409836064
best_threshold: 0.99 ， best_f1: 0.2145922746781116
loss tensor(1.1903, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.63 ， best_f1: 0.38967527060782686
best_threshold: 0.88 ， best_f1: 0.21311475409836064
best_threshold: 0.99 ， best_f1: 0.21645021645021648
loss tensor(1.2429, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.79 ， best_f1: 0.33819241982507287
best_threshold: 0.99 ， best_f1: 0.15789473684210528
best_threshold: 0.99 ， best_f1: 0.1917808219178082
loss tensor(1.1902, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.59 ， best_f1: 0.3405088062622309
best_threshold: 0.93 ， best_f1: 0.1565217391304348
best_threshold: 0.99 ， best_f1: 0.1917808219178082
loss tensor(1.3162, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.53 ， best_f1: 0.3405088062622309
best_threshold: 0.9500000000000001 ， best_f1: 0.1565217391304348
best_threshold: 0.99 ， best_f1: 0.1917808219178082
loss tensor(1.2497, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.53 ， best_f1: 0.3405088062622309
best_threshold: 0.97 ， best_f1: 0.1565217391304348
best_threshold: 0.99 ， best_f1: 0.1917808219178082
loss tensor(1.1898, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.52 ， best_f1: 0.3418467583497053
best_threshold: 0.98 ， best_f1: 0.1565217391304348
best_threshold: 0.99 ， best_f1: 0.19090909090909092
loss tensor(1.1892, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.52 ， best_f1: 0.3448959365708622
best_threshold: 0.98 ， best_f1: 0.1565217391304348
best_threshold: 0.99 ， best_f1: 0.19090909090909092
loss tensor(1.2111, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.52 ， best_f1: 0.3448959365708622
best_threshold: 0.91 ， best_f1: 0.15517241379310345
best_threshold: 0.99 ， best_f1: 0.19090909090909092
loss tensor(1.1894, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.34558093346573987
best_threshold: 0.93 ， best_f1: 0.15517241379310345
best_threshold: 0.99 ， best_f1: 0.1926605504587156
loss tensor(1.2406, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.34592445328031807
best_threshold: 0.88 ， best_f1: 0.15384615384615383
best_threshold: 0.97 ， best_f1: 0.1883408071748879
loss tensor(1.2401, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.34592445328031807
best_threshold: 0.5800000000000001 ， best_f1: 0.15126050420168066
best_threshold: 0.36000000000000004 ， best_f1: 0.1796875
loss tensor(1.2408, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.1697792869269949
best_threshold: 0.04 ， best_f1: 0.12121212121212122
best_threshold: 0.02 ， best_f1: 0.08547008547008546
loss tensor(1.2400, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.1697792869269949
best_threshold: 0.98 ， best_f1: 0.1111111111111111
best_threshold: 0.74 ， best_f1: 0.07960199004975124
loss tensor(1.2402, device='cuda:1',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
best_threshold: 0.51 ， best_f1: 0.17035775127768316
best_threshold: 0.98 ， best_f1: 0.1111111111111111
best_threshold: 0.74 ， best_f1: 0.07960199004975124
此时的y值：tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
NaN values found in the forward output y!
loss tensor(nan, device='cuda:1', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
此时的y值：tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
NaN values found in the forward output y!
pred_y中出现了Nan
此时的pred_y值：[tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
best_threshold: 0.5 ， best_f1: 0
Traceback (most recent call last):
  File "sbgnn.py", line 523, in <module>
    main()
  File "sbgnn.py", line 520, in main
    run()
  File "sbgnn.py", line 495, in run
    res = test_and_val(pred_y, train_y, mode='train', epoch=epoch)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "sbgnn.py", line 416, in test_and_val
    auc = roc_auc_score(test_y, preds)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/_param_validation.py", line 214, in wrapper
    return func(*args, **kwargs)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/metrics/_ranking.py", line 606, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/validation.py", line 957, in check_array
    _assert_all_finite(
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/validation.py", line 122, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/home/yrgu/miniconda3/envs/experiment/lib/python3.8/site-packages/sklearn/utils/validation.py", line 171, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.
