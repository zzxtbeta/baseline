sbgnn.py --lr 5e-3 --seed 2023 --dataset_name bitcoin_alpha-1 --gnn_layer 2 --epoch 300 --agg AttentionAggregator
#!/usr/bin/env python3
#-*- coding: utf-8 -*-
"""
@author: huangjunjie
@file: sbgnn.py
@time: 2021/03/28
"""


import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
import sys
import time
import random
import argparse
import subprocess

from collections import defaultdict

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, roc_auc_score


from tqdm import tqdm

import logging
# https://docs.python.org/3/howto/logging.html#logging-advanced-tutorial


BASE_DIR = os.path.dirname(os.path.abspath(__file__))

parser = argparse.ArgumentParser()
parser.add_argument('--dirpath', default=BASE_DIR, help='Current Dir')
parser.add_argument('--device', type=str, default='cuda:0', help='Devices')
parser.add_argument('--dataset_name', type=str, default='review-2')
parser.add_argument('--a_emb_size', type=int, default=32, help='Embeding A Size')
parser.add_argument('--b_emb_size', type=int, default=32, help='Embeding B Size')
parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight Decay')
parser.add_argument('--lr', type=float, default=0.005, help='Learning Rate')
parser.add_argument('--seed', type=int, default=2023, help='Random seed')
parser.add_argument('--epoch', type=int, default=2000, help='Epoch')
parser.add_argument('--gnn_layer_num', type=int, default=2, help='GNN Layer')
parser.add_argument('--batch_size', type=int, default=500, help='Batch Size')
parser.add_argument('--dropout', type=float, default=0.5, help='Dropout')
parser.add_argument('--agg', type=str, default='AttentionAggregator', choices=['AttentionAggregator', 'MeanAggregator'], help='Aggregator')
args = parser.parse_args()


# TODO

exclude_hyper_params = ['dirpath', 'device']
hyper_params = dict(vars(args))
for exclude_p in exclude_hyper_params:
    del hyper_params[exclude_p]

hyper_params = "~".join([f"{k}-{v}" for k,v in hyper_params.items()])

from torch.utils.tensorboard import SummaryWriter
# https://pytorch.org/docs/stable/tensorboard.html
tb_writer = SummaryWriter(comment=hyper_params)


def setup_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
# setup seed
setup_seed(args.seed)

from common import DATA_EMB_DIC

# args.device = 'cpu'
args.device = torch.device(args.device)

class MeanAggregator(nn.Module):
    def __init__(self, a_dim, b_dim):
        super(MeanAggregator, self).__init__()

        self.out_mlp_layer = nn.Sequential(
            nn.Linear(b_dim, b_dim)
        )

    def forward(self, edge_dic_list: dict, feature_a, feature_b, node_num_a, node_num_b):

        edges = []
        for node in range(node_num_a):
            neighs = np.array(edge_dic_list[node]).reshape(-1, 1)
            a = np.array([node]).repeat(len(neighs)).reshape(-1, 1)
            edges.append(np.concatenate([a, neighs], axis=1))

        edges = np.vstack(edges)
        edges = torch.LongTensor(edges).to(args.device)
        matrix = torch.sparse_coo_tensor(edges.t(), torch.ones(edges.shape[0]), torch.Size([node_num_a, node_num_b]), device=args.device)
        row_sum = torch.spmm(matrix, torch.ones(size=(node_num_b, 1)).to(args.device))
        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(args.device), row_sum)

        new_emb = feature_b
        new_emb = self.out_mlp_layer(new_emb)
        output_emb = torch.spmm(matrix, new_emb)
        output_emb = output_emb.div(row_sum)

        return output_emb


class AttentionAggregator(nn.Module):
    def __init__(self, a_dim, b_dim):
        super(AttentionAggregator, self).__init__()

        self.out_mlp_layer = nn.Sequential(
            nn.Linear(b_dim, b_dim),
        )

        self.a = nn.Parameter(torch.FloatTensor(a_dim + b_dim, 1))
        nn.init.kaiming_normal_(self.a.data)

    def forward(self, edge_dic_list: dict, feature_a, feature_b, node_num_a, node_num_b):

        edges = []
        for node in range(node_num_a):
            neighs = np.array(edge_dic_list[node]).reshape(-1, 1)
            a = np.array([node]).repeat(len(neighs)).reshape(-1, 1)
            edges.append(np.concatenate([a, neighs], axis=1))

        edges = np.vstack(edges)
        edges = torch.LongTensor(edges).to(args.device)

        new_emb = feature_b
        new_emb = self.out_mlp_layer(new_emb)

        # print("=======139=====================================139================================139==============")
        # print(f"edge.shape:{edges.shape}")
        # print("Edges[:, 0]:", edges[:, 0].shape)
        # print("Edges[:, 1]:", edges[:, 1].shape)
        # print("===================================================================================================")
        # print("Max index in Edges[:, 0]:", torch.max(edges[:, 0]))
        # print("Min index in Edges[:, 0]:", torch.min(edges[:, 0]))
        edge_h_2 = torch.cat([feature_a[edges[:, 0]], new_emb[edges[:, 1]] ], dim=1)
        edges_h = torch.exp(F.elu(torch.einsum("ij,jl->il", [edge_h_2, self.a]), 0.1))

        matrix = torch.sparse_coo_tensor(edges.t(), edges_h[:, 0], torch.Size([node_num_a, node_num_b]), device=args.device)
        row_sum = torch.sparse.mm(matrix, torch.ones(size=(node_num_b, 1)).to(args.device))
        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(args.device), row_sum)

        output_emb = torch.sparse.mm(matrix, new_emb)
        output_emb = output_emb.div(row_sum)
        return output_emb



class SBGNNLayer(nn.Module):
    def __init__(self, edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg, \
        dataset_name=args.dataset_name, emb_size_a=32, emb_size_b=32, aggregator=MeanAggregator):
        super(SBGNNLayer, self).__init__()
        #
        self.set_a_num, self.set_b_num = DATA_EMB_DIC[dataset_name]

        # self.feature_a = feature_a
        # self.feature_b = feature_b
        self.edgelist_a_b_pos, self.edgelist_a_b_neg, self.edgelist_b_a_pos, self.edgelist_b_a_neg = \
            edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg
        self.edgelist_a_a_pos, self.edgelist_a_a_neg, self.edgelist_b_b_pos, self.edgelist_b_b_neg = \
            edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg

        self.agg_a_from_b_pos = aggregator(emb_size_b, emb_size_a)
        self.agg_a_from_b_neg = aggregator(emb_size_b, emb_size_a)
        self.agg_a_from_a_pos = aggregator(emb_size_a, emb_size_a)
        self.agg_a_from_a_neg = aggregator(emb_size_a, emb_size_a)

        self.agg_b_from_a_pos = aggregator(emb_size_a, emb_size_b)
        self.agg_b_from_a_neg = aggregator(emb_size_a, emb_size_b)
        self.agg_b_from_b_pos = aggregator(emb_size_b, emb_size_b)
        self.agg_b_from_b_neg = aggregator(emb_size_b, emb_size_b)

        self.update_func = nn.Sequential(
            nn.Dropout(args.dropout),
            nn.Linear(emb_size_a * 5, emb_size_a * 2),
            nn.PReLU(),
            nn.Linear(emb_size_b * 2, emb_size_b)

        )



    def forward(self, feature_a, feature_b):
        # assert feature_a.size()[0] == self.set_a_num, 'set_b_num error'
        # assert feature_b.size()[0] == self.set_b_num, 'set_b_num error'

        node_num_a, node_num_b = self.set_a_num, self.set_b_num

        m_a_from_b_pos = self.agg_a_from_b_pos(self.edgelist_a_b_pos, feature_a, feature_b, node_num_a, node_num_b)
        m_a_from_b_neg = self.agg_a_from_b_neg(self.edgelist_a_b_neg, feature_a, feature_b, node_num_a, node_num_b)
        m_a_from_a_pos = self.agg_a_from_a_pos(self.edgelist_a_a_pos, feature_a, feature_a, node_num_a, node_num_a)
        m_a_from_a_neg = self.agg_a_from_a_neg(self.edgelist_a_a_neg, feature_a, feature_a, node_num_a, node_num_a)

        new_feature_a = torch.cat([feature_a, m_a_from_b_pos, m_a_from_b_neg, m_a_from_a_pos, m_a_from_a_neg], dim=1)
        new_feature_a = self.update_func(new_feature_a)

        m_b_from_a_pos = self.agg_b_from_a_pos(self.edgelist_b_a_pos, feature_b, feature_a, node_num_b, node_num_a)
        m_b_from_a_neg = self.agg_b_from_a_neg(self.edgelist_b_a_neg, feature_b, feature_a, node_num_b, node_num_a)
        m_b_from_b_pos = self.agg_b_from_b_pos(self.edgelist_b_b_pos, feature_b, feature_b, node_num_b, node_num_b)
        m_b_from_b_neg = self.agg_b_from_b_neg(self.edgelist_b_b_neg, feature_b, feature_b, node_num_b, node_num_b)

        new_feature_b = torch.cat([feature_b, m_b_from_a_pos, m_b_from_a_neg, m_b_from_b_pos, m_b_from_b_neg], dim=1)
        new_feature_b = self.update_func(new_feature_b)

        return new_feature_a, new_feature_b



class SBGNN(nn.Module):
    def __init__(self, edgelists,
                    dataset_name=args.dataset_name, layer_num=1, emb_size_a=32, emb_size_b=32, aggregator=AttentionAggregator):
        super(SBGNN, self).__init__()

        # assert edgelists must compelte
        assert len(edgelists) == 8, 'must 8 edgelists'
        edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg = edgelists

        self.set_a_num, self.set_b_num = DATA_EMB_DIC[dataset_name]

        self.features_a = nn.Embedding(self.set_a_num, emb_size_a)
        self.features_b = nn.Embedding(self.set_b_num, emb_size_b)
        self.features_a.weight.requires_grad = True
        self.features_b.weight.requires_grad = True
        # features_a = features_a.to(args.device)
        # features_b = features_b.to(args.device)

        self.layers = nn.ModuleList(
            [SBGNNLayer(edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg, \
                    dataset_name=dataset_name, emb_size_a=32, emb_size_b=32, aggregator=aggregator) for _ in range(layer_num)]
        )
        # self.mlp = nn.Sequential(
        #     nn.Linear(emb_size_a * 3, 30),
        #     nn.PReLU(),
        #     nn.Linear(30, 1),
        #     nn.Sigmoid()
        # )
        # def init_weights(m):
        #     if type(m) == nn.Linear:
        #         torch.nn.init.xavier_uniform_(m.weight)
        #         m.bias.data.fill_(0.01)
        # self.apply(init_weights)


    def get_embeddings(self):
        emb_a = self.features_a(torch.arange(self.set_a_num).to(args.device))
        emb_b = self.features_b(torch.arange(self.set_b_num).to(args.device))
        for m in self.layers:
            emb_a, emb_b = m(emb_a, emb_b)
        return emb_a, emb_b

    def forward(self, edge_lists):
        embedding_a, embedding_b = self.get_embeddings()

        #### with mlp
        # emb_concat = torch.cat([embedding_a[edge_lists[:, 0]], embedding_b[edge_lists[:, 1]], embedding_a[edge_lists[:, 0]] * embedding_b[edge_lists[:, 1]] ], dim=1)
        # y = self.mlp(emb_concat).squeeze(-1)
        # return y

        ## without mlp
        y = torch.einsum("ij, ij->i", [embedding_a[edge_lists[:, 0]] , embedding_b[edge_lists[:, 1]] ])
        y = torch.sigmoid(y)  # 添加sigmoid激活函数
        return y

    def loss(self, pred_y, y):
        assert y.min() >= 0, 'must 0~1'
        assert pred_y.size() == y.size(), 'must be same length'
        pos_ratio = y.sum() /  y.size()[0]
        weight = torch.where(y > 0.5, 1./pos_ratio, 1./(1-pos_ratio))
        # weight = torch.where(y > 0.5, (1-pos_ratio), pos_ratio)
        return F.binary_cross_entropy(pred_y, y, weight=weight)


# =========== function
def load_data(dataset_name):
    train_file_path = os.path.join('experiments-data', f'{dataset_name}_training.txt')
    val_file_path = os.path.join('experiments-data', f'{dataset_name}_validation.txt')
    test_file_path = os.path.join('experiments-data', f'{dataset_name}_testing.txt')

    train_edgelist = []
    with open(train_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s = map(int, line.split('\t'))
            train_edgelist.append((a, b, s))

    val_edgelist = []
    with open(val_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s = map(int, line.split('\t'))
            val_edgelist.append((a, b, s))

    test_edgelist = []
    with open(test_file_path) as f:
        for ind, line in enumerate(f):
            if ind == 0: continue
            a, b, s = map(int, line.split('\t'))
            test_edgelist.append((a, b, s))

    return np.array(train_edgelist), np.array(val_edgelist), np.array(test_edgelist)


# ============= load data
def load_edgelists(edge_lists):
    edgelist_a_b_pos, edgelist_a_b_neg = defaultdict(list), defaultdict(list)
    edgelist_b_a_pos, edgelist_b_a_neg = defaultdict(list), defaultdict(list)
    edgelist_a_a_pos, edgelist_a_a_neg = defaultdict(list), defaultdict(list)
    edgelist_b_b_pos, edgelist_b_b_neg = defaultdict(list), defaultdict(list)

    for a, b, s in edge_lists:
        if s == 1:
            edgelist_a_b_pos[a].append(b)
            edgelist_b_a_pos[b].append(a)
        elif s== -1:
            edgelist_a_b_neg[a].append(b)
            edgelist_b_a_neg[b].append(a)
        else:
            print(a, b, s)
            raise Exception("s must be -1/1")

    edge_list_a_a = defaultdict(lambda: defaultdict(int))
    edge_list_b_b = defaultdict(lambda: defaultdict(int))
    for a, b, s in edge_lists:
        for b2 in edgelist_a_b_pos[a]:
            edge_list_b_b[b][b2] += 1 * s
        for b2 in edgelist_a_b_neg[a]:
            edge_list_b_b[b][b2] -= 1 * s
        for a2 in edgelist_b_a_pos[b]:
            edge_list_a_a[a][a2] += 1 * s
        for a2 in edgelist_b_a_neg[b]:
            edge_list_a_a[a][a2] -= 1 * s

    for a1 in edge_list_a_a:
        for a2 in edge_list_a_a[a1]:
            v = edge_list_a_a[a1][a2]
            if a1 == a2: continue
            if v > 0:
                edgelist_a_a_pos[a1].append(a2)
            elif v < 0:
                edgelist_a_a_neg[a1].append(a2)

    for b1 in edge_list_b_b:
        for b2 in edge_list_b_b[b1]:
            v = edge_list_b_b[b1][b2]
            if b1 == b2: continue
            if v > 0:
                edgelist_b_b_pos[b1].append(b2)
            elif v < 0:
                edgelist_b_b_neg[b1].append(b2)

    return edgelist_a_b_pos, edgelist_a_b_neg, edgelist_b_a_pos, edgelist_b_a_neg,\
                    edgelist_a_a_pos, edgelist_a_a_neg, edgelist_b_b_pos, edgelist_b_b_neg


@torch.no_grad()
def test_and_val(pred_y, y, mode='val', epoch=0):
    preds = pred_y.cpu().numpy()
    y = y.cpu().numpy()

    preds[preds >= 0.5]  = 1
    preds[preds < 0.5] = 0
    test_y = y

    auc = roc_auc_score(test_y, preds)
    f1 = f1_score(test_y, preds)
    macro_f1 = f1_score(test_y, preds, average='macro')
    micro_f1 = f1_score(test_y, preds, average='micro')
    pos_ratio = np.sum(test_y) /  len(test_y)
    res = {
        f'{mode}_auc': auc,
        f'{mode}_f1' : f1,
        f'{mode}_pos_ratio': pos_ratio,
        f'{mode}_epoch': epoch,
        f'{mode}_macro_f1' : macro_f1,
        f'{mode}_micro_f1' : micro_f1,
    }
    for k, v in res.items():
        mode ,_, metric = k.partition('_')
        tb_writer.add_scalar(f'{metric}/{mode}', v, epoch)
    # tb_writer.add_scalar( f'{mode}_auc', auc, epoch)
    # tb_writer.add_scalar( f'{mode}_f1', auc, epoch)
    return res



def run():
    train_edgelist, val_edgelist, test_edgelist  = load_data(args.dataset_name)

    set_a_num, set_b_num = DATA_EMB_DIC[args.dataset_name]
    train_y = np.array([i[-1] for i in train_edgelist])
    val_y   = np.array([i[-1] for i in val_edgelist])
    test_y  = np.array([i[-1] for i in test_edgelist])

    train_y = torch.from_numpy( (train_y + 1)/2 ).float().to(args.device)
    val_y = torch.from_numpy( (val_y + 1)/2 ).float().to(args.device)
    test_y = torch.from_numpy( (test_y + 1)/2 ).float().to(args.device)
    # get edge lists
    edgelists = load_edgelists(train_edgelist)

    if args.agg == 'MeanAggregator':
        agg = MeanAggregator
    else:
        agg = AttentionAggregator

    model = SBGNN(edgelists, dataset_name=args.dataset_name, layer_num=args.gnn_layer_num, aggregator=agg)
    model = model.to(args.device)

    print(model.train())
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    res_best = {'val_auc': 0}
    for epoch in tqdm(range(1, args.epoch + 2)):
        # train
        model.train()
        optimizer.zero_grad()
        pred_y = model(train_edgelist)
        loss = model.loss(pred_y, train_y)
        loss.backward()
        optimizer.step()
        print('loss', loss)


        res_cur = {}
        # if epoch % 5 == 0:
        if True:
        # val/test
            model.eval()
            pred_y = model(train_edgelist)
            res = test_and_val(pred_y, train_y, mode='train', epoch=epoch)
            res_cur.update(res)
            pred_val_y = model(val_edgelist)
            res = test_and_val(pred_val_y, val_y, mode='val', epoch=epoch)
            res_cur.update(res)
            pred_test_y = model(test_edgelist)
            res = test_and_val(pred_test_y, test_y, mode='test', epoch=epoch)
            res_cur.update(res)
            if res_cur['val_auc'] > res_best['val_auc']:
                res_best = res_cur
                print(res_best)
    print('Done! Best Results:')
    print(res_best)
    print_list = ['test_auc', 'test_f1', 'test_macro_f1', 'test_micro_f1']
    for i in print_list:
        print(i, res_best[i], end=' ')



def main():
    print(" ".join(sys.argv))
    this_fpath = os.path.abspath(__file__)
    t = subprocess.run(f'cat {this_fpath}', shell=True, stdout=subprocess.PIPE)
    print(str(t.stdout, 'utf-8'))
    print('=' * 20)
    run()

if __name__ == "__main__":
    main()

====================
SBGNN(
  (features_a): Embedding(3783, 32)
  (features_b): Embedding(3783, 32)
  (layers): ModuleList(
    (0-1): 2 x SBGNNLayer(
      (agg_a_from_b_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_b_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_a_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_a_from_a_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_a_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_a_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_b_pos): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (agg_b_from_b_neg): AttentionAggregator(
        (out_mlp_layer): Sequential(
          (0): Linear(in_features=32, out_features=32, bias=True)
        )
      )
      (update_func): Sequential(
        (0): Dropout(p=0.5, inplace=False)
        (1): Linear(in_features=160, out_features=64, bias=True)
        (2): PReLU(num_parameters=1)
        (3): Linear(in_features=64, out_features=32, bias=True)
      )
    )
  )
)

  0%|          | 0/301 [00:00<?, ?it/s]
  0%|          | 1/301 [00:19<1:35:26, 19.09s/it]
  1%|          | 2/301 [00:33<1:21:11, 16.29s/it]
  1%|          | 3/301 [00:48<1:17:27, 15.60s/it]
  1%|▏         | 4/301 [01:02<1:14:52, 15.13s/it]
  2%|▏         | 5/301 [01:16<1:12:15, 14.65s/it]
  2%|▏         | 6/301 [01:30<1:11:37, 14.57s/it]
  2%|▏         | 7/301 [01:45<1:10:54, 14.47s/it]
  3%|▎         | 8/301 [01:59<1:10:22, 14.41s/it]
  3%|▎         | 9/301 [02:12<1:08:54, 14.16s/it]
  3%|▎         | 10/301 [02:27<1:08:59, 14.23s/it]
  4%|▎         | 11/301 [02:41<1:08:09, 14.10s/it]
  4%|▍         | 12/301 [02:55<1:08:26, 14.21s/it]
  4%|▍         | 13/301 [03:09<1:07:06, 13.98s/it]
  5%|▍         | 14/301 [03:21<1:04:26, 13.47s/it]
  5%|▍         | 15/301 [03:34<1:04:00, 13.43s/it]
  5%|▌         | 16/301 [03:47<1:03:23, 13.35s/it]
  6%|▌         | 17/301 [04:02<1:04:55, 13.72s/it]
  6%|▌         | 18/301 [04:16<1:05:47, 13.95s/it]
  6%|▋         | 19/301 [04:30<1:05:30, 13.94s/it]
  7%|▋         | 20/301 [04:44<1:05:02, 13.89s/it]
  7%|▋         | 21/301 [04:58<1:04:18, 13.78s/it]
  7%|▋         | 22/301 [05:12<1:04:40, 13.91s/it]
  8%|▊         | 23/301 [05:26<1:04:24, 13.90s/it]
  8%|▊         | 24/301 [05:39<1:03:33, 13.77s/it]
  8%|▊         | 25/301 [05:54<1:04:26, 14.01s/it]
  9%|▊         | 26/301 [06:08<1:04:57, 14.17s/it]
  9%|▉         | 27/301 [06:23<1:05:21, 14.31s/it]
  9%|▉         | 28/301 [06:38<1:05:55, 14.49s/it]
 10%|▉         | 29/301 [06:52<1:05:43, 14.50s/it]
 10%|▉         | 30/301 [07:07<1:05:09, 14.43s/it]
 10%|█         | 31/301 [07:20<1:04:09, 14.26s/it]
 11%|█         | 32/301 [07:34<1:02:20, 13.91s/it]
 11%|█         | 33/301 [07:48<1:02:18, 13.95s/it]
 11%|█▏        | 34/301 [07:57<56:05, 12.60s/it]  
 12%|█▏        | 35/301 [08:06<50:52, 11.47s/it]
 12%|█▏        | 36/301 [08:14<46:07, 10.44s/it]
 12%|█▏        | 37/301 [08:32<56:16, 12.79s/it]
 13%|█▎        | 38/301 [08:55<1:09:13, 15.79s/it]
 13%|█▎        | 39/301 [09:18<1:17:47, 17.82s/it]
 13%|█▎        | 40/301 [09:40<1:23:26, 19.18s/it]
 14%|█▎        | 41/301 [10:03<1:27:34, 20.21s/it]
 14%|█▍        | 42/301 [10:24<1:29:08, 20.65s/it]
 14%|█▍        | 43/301 [10:46<1:30:13, 20.98s/it]
 15%|█▍        | 44/301 [11:08<1:30:59, 21.24s/it]
 15%|█▍        | 45/301 [11:29<1:30:22, 21.18s/it]
 15%|█▌        | 46/301 [11:51<1:31:09, 21.45s/it]
 16%|█▌        | 47/301 [12:13<1:31:28, 21.61s/it]
 16%|█▌        | 48/301 [12:35<1:31:05, 21.60s/it]
 16%|█▋        | 49/301 [12:55<1:29:34, 21.33s/it]
 17%|█▋        | 50/301 [13:16<1:28:29, 21.15s/it]
 17%|█▋        | 51/301 [13:38<1:28:40, 21.28s/it]
 17%|█▋        | 52/301 [13:59<1:29:09, 21.48s/it]
 18%|█▊        | 53/301 [14:20<1:28:01, 21.30s/it]
 18%|█▊        | 54/301 [14:42<1:28:01, 21.38s/it]
 18%|█▊        | 55/301 [15:04<1:28:58, 21.70s/it]
 19%|█▊        | 56/301 [15:25<1:26:50, 21.27s/it]
 19%|█▉        | 57/301 [15:45<1:25:51, 21.11s/it]
 19%|█▉        | 58/301 [16:06<1:25:21, 21.07s/it]
 20%|█▉        | 59/301 [16:27<1:24:17, 20.90s/it]
 20%|█▉        | 60/301 [16:47<1:23:23, 20.76s/it]
 20%|██        | 61/301 [17:09<1:23:53, 20.97s/it]
 21%|██        | 62/301 [17:30<1:23:16, 20.90s/it]
 21%|██        | 63/301 [17:50<1:22:34, 20.82s/it]
 21%|██▏       | 64/301 [18:11<1:21:52, 20.73s/it]
 22%|██▏       | 65/301 [18:31<1:20:35, 20.49s/it]
 22%|██▏       | 66/301 [18:50<1:19:08, 20.21s/it]
 22%|██▏       | 67/301 [19:09<1:17:32, 19.88s/it]
 23%|██▎       | 68/301 [19:29<1:16:40, 19.74s/it]
 23%|██▎       | 69/301 [19:49<1:16:47, 19.86s/it]
 23%|██▎       | 70/301 [20:07<1:14:38, 19.39s/it]
 24%|██▎       | 71/301 [20:25<1:12:13, 18.84s/it]
 24%|██▍       | 72/301 [20:45<1:13:31, 19.26s/it]
 24%|██▍       | 73/301 [21:04<1:13:13, 19.27s/it]
 25%|██▍       | 74/301 [21:24<1:13:02, 19.31s/it]
 25%|██▍       | 75/301 [21:43<1:12:54, 19.36s/it]
 25%|██▌       | 76/301 [22:02<1:12:35, 19.36s/it]
 26%|██▌       | 77/301 [22:22<1:12:52, 19.52s/it]loss tensor(1.4011, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
{'train_auc': 0.5, 'train_f1': 0.9675572519083969, 'train_pos_ratio': 0.9371534195933456, 'train_epoch': 1, 'train_macro_f1': 0.48377862595419846, 'train_micro_f1': 0.9371534195933456, 'val_auc': 0.5, 'val_f1': 0.9675213675213675, 'val_pos_ratio': 0.9370860927152318, 'val_epoch': 1, 'val_macro_f1': 0.48376068376068376, 'val_micro_f1': 0.9370860927152318, 'test_auc': 0.5, 'test_f1': 0.9639948564080583, 'test_pos_ratio': 0.9304923458833264, 'test_epoch': 1, 'test_macro_f1': 0.48199742820402913, 'test_micro_f1': 0.9304923458833264}
loss tensor(1.3749, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
{'train_auc': 0.8150757811688986, 'train_f1': 0.800856663976659, 'train_pos_ratio': 0.9371534195933456, 'train_epoch': 2, 'train_macro_f1': 0.5398978757060189, 'train_micro_f1': 0.6879073839867691, 'val_auc': 0.7489306304630835, 'val_f1': 0.813857290589452, 'val_pos_ratio': 0.9370860927152318, 'val_epoch': 2, 'val_macro_f1': 0.5334846618922364, 'val_micro_f1': 0.7019867549668874, 'test_auc': 0.7187943318723665, 'test_f1': 0.8019879675647397, 'test_pos_ratio': 0.9304923458833264, 'test_epoch': 2, 'test_macro_f1': 0.5266121835845459, 'test_micro_f1': 0.6868018204385602}
loss tensor(1.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(1.1649, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.7945, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.8634, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(3.8723, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.8176, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.7614, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(1.1649, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.9559, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.8729, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.8789, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.8767, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.8101, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.7767, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.7549, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.7478, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.6923, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.7153, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.5667, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.5179, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
{'train_auc': 0.9298673066236772, 'train_f1': 0.950825010855406, 'train_pos_ratio': 0.9371534195933456, 'train_epoch': 22, 'train_macro_f1': 0.7631350921193618, 'train_micro_f1': 0.9118591302655901, 'val_auc': 0.7513948298307607, 'val_f1': 0.9329660238751148, 'val_pos_ratio': 0.9370860927152318, 'val_epoch': 22, 'val_macro_f1': 0.6597603228619271, 'val_micro_f1': 0.8791390728476821, 'test_auc': 0.739865866310953, 'test_f1': 0.9301038062283737, 'test_pos_ratio': 0.9304923458833264, 'test_epoch': 22, 'test_macro_f1': 0.6614446886853291, 'test_micro_f1': 0.8746379809681423}
loss tensor(0.5182, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
{'train_auc': 0.9394353661738754, 'train_f1': 0.9416801602766419, 'train_pos_ratio': 0.9371534195933456, 'train_epoch': 23, 'train_macro_f1': 0.7437616445751665, 'train_micro_f1': 0.8966339138048448, 'val_auc': 0.7702017853821834, 'val_f1': 0.9262865090403337, 'val_pos_ratio': 0.9370860927152318, 'val_epoch': 23, 'val_macro_f1': 0.6561934475703599, 'val_micro_f1': 0.8683774834437086, 'test_auc': 0.7736149929069005, 'test_f1': 0.9210957621166004, 'test_pos_ratio': 0.9304923458833264, 'test_epoch': 23, 'test_macro_f1': 0.6612583606320125, 'test_micro_f1': 0.8605709557302441}
loss tensor(0.5131, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.4670, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.4223, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.4120, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.4148, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3969, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3964, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3764, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3853, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3680, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3326, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3387, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3242, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3300, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2892, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2859, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3015, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.3100, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2795, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2725, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2660, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2586, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2610, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2588, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2595, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2619, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2553, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2493, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2371, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2412, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2412, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2386, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2382, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2434, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2348, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2484, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2270, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2254, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2300, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2238, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2270, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2144, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2088, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2146, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2254, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2149, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2141, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2100, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2109, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2096, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2063, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss 
 26%|██▌       | 78/301 [22:41<1:11:59, 19.37s/it]
 26%|██▌       | 79/301 [23:01<1:11:57, 19.45s/it]
 27%|██▋       | 80/301 [23:21<1:12:05, 19.57s/it]
 27%|██▋       | 81/301 [23:40<1:11:39, 19.54s/it]
 27%|██▋       | 82/301 [24:00<1:11:29, 19.59s/it]
 28%|██▊       | 83/301 [24:20<1:11:05, 19.56s/it]
 28%|██▊       | 84/301 [24:39<1:10:22, 19.46s/it]
 28%|██▊       | 85/301 [24:57<1:09:17, 19.25s/it]
 29%|██▊       | 86/301 [25:17<1:08:45, 19.19s/it]
 29%|██▉       | 87/301 [25:36<1:08:28, 19.20s/it]
 29%|██▉       | 88/301 [25:56<1:09:17, 19.52s/it]
 30%|██▉       | 89/301 [26:15<1:08:02, 19.26s/it]
 30%|██▉       | 90/301 [26:33<1:06:52, 19.02s/it]
 30%|███       | 91/301 [26:51<1:05:29, 18.71s/it]
 31%|███       | 92/301 [27:09<1:04:33, 18.53s/it]
 31%|███       | 93/301 [27:28<1:04:01, 18.47s/it]
 31%|███       | 94/301 [27:47<1:04:17, 18.64s/it]
 32%|███▏      | 95/301 [28:06<1:05:08, 18.97s/it]
 32%|███▏      | 96/301 [28:25<1:04:36, 18.91s/it]
 32%|███▏      | 97/301 [28:44<1:04:30, 18.97s/it]
 33%|███▎      | 98/301 [29:03<1:04:14, 18.99s/it]
 33%|███▎      | 99/301 [29:24<1:05:18, 19.40s/it]
 33%|███▎      | 100/301 [29:43<1:04:54, 19.37s/it]
 34%|███▎      | 101/301 [30:02<1:04:21, 19.31s/it]
 34%|███▍      | 102/301 [30:21<1:03:59, 19.29s/it]
 34%|███▍      | 103/301 [30:41<1:03:41, 19.30s/it]
 35%|███▍      | 104/301 [31:01<1:04:04, 19.52s/it]
 35%|███▍      | 105/301 [31:20<1:03:25, 19.42s/it]
 35%|███▌      | 106/301 [31:40<1:03:25, 19.51s/it]
 36%|███▌      | 107/301 [31:58<1:02:19, 19.27s/it]
 36%|███▌      | 108/301 [32:17<1:01:12, 19.03s/it]
 36%|███▌      | 109/301 [32:36<1:00:55, 19.04s/it]
 37%|███▋      | 110/301 [32:56<1:01:18, 19.26s/it]
 37%|███▋      | 111/301 [33:16<1:01:39, 19.47s/it]
 37%|███▋      | 112/301 [33:35<1:01:05, 19.39s/it]
 38%|███▊      | 113/301 [33:55<1:01:38, 19.67s/it]
 38%|███▊      | 114/301 [34:15<1:01:28, 19.72s/it]
 38%|███▊      | 115/301 [34:34<1:00:50, 19.63s/it]
 39%|███▊      | 116/301 [34:53<59:20, 19.25s/it]  
 39%|███▉      | 117/301 [35:11<57:54, 18.89s/it]
 39%|███▉      | 118/301 [35:28<56:29, 18.52s/it]
 40%|███▉      | 119/301 [35:45<54:06, 17.84s/it]
 40%|███▉      | 120/301 [36:03<54:14, 17.98s/it]
 40%|████      | 121/301 [36:21<53:39, 17.88s/it]
 41%|████      | 122/301 [36:39<53:25, 17.91s/it]
 41%|████      | 123/301 [36:56<52:57, 17.85s/it]
 41%|████      | 124/301 [37:13<51:30, 17.46s/it]
 42%|████▏     | 125/301 [37:30<50:53, 17.35s/it]
 42%|████▏     | 126/301 [37:48<51:13, 17.56s/it]
 42%|████▏     | 127/301 [38:06<51:44, 17.84s/it]
 43%|████▎     | 128/301 [38:25<51:43, 17.94s/it]
 43%|████▎     | 129/301 [38:42<50:43, 17.70s/it]
 43%|████▎     | 130/301 [39:00<50:46, 17.82s/it]
 44%|████▎     | 131/301 [39:18<50:47, 17.93s/it]
 44%|████▍     | 132/301 [39:36<50:23, 17.89s/it]
 44%|████▍     | 133/301 [39:53<49:37, 17.73s/it]
 45%|████▍     | 134/301 [40:11<49:08, 17.66s/it]
 45%|████▍     | 135/301 [40:29<49:07, 17.76s/it]
 45%|████▌     | 136/301 [40:45<47:50, 17.40s/it]
 46%|████▌     | 137/301 [41:03<47:37, 17.42s/it]
 46%|████▌     | 138/301 [41:20<46:49, 17.24s/it]
 46%|████▌     | 139/301 [41:36<45:56, 17.02s/it]
 47%|████▋     | 140/301 [41:53<45:45, 17.05s/it]
 47%|████▋     | 141/301 [42:10<45:33, 17.09s/it]
 47%|████▋     | 142/301 [42:28<45:20, 17.11s/it]
 48%|████▊     | 143/301 [42:43<44:09, 16.77s/it]
 48%|████▊     | 144/301 [43:01<44:44, 17.10s/it]
 48%|████▊     | 145/301 [43:19<45:02, 17.32s/it]
 49%|████▊     | 146/301 [43:36<44:02, 17.05s/it]
 49%|████▉     | 147/301 [43:53<43:49, 17.07s/it]
 49%|████▉     | 148/301 [44:11<44:06, 17.30s/it]
 50%|████▉     | 149/301 [44:27<42:59, 16.97s/it]
 50%|████▉     | 150/301 [44:45<43:36, 17.33s/it]
 50%|█████     | 151/301 [45:01<42:44, 17.10s/it]
 50%|█████     | 152/301 [45:18<42:11, 16.99s/it]
 51%|█████     | 153/301 [45:36<42:36, 17.28s/it]
 51%|█████     | 154/301 [45:53<41:49, 17.07s/it]
 51%|█████▏    | 155/301 [46:09<40:45, 16.75s/it]
 52%|█████▏    | 156/301 [46:25<40:06, 16.59s/it]
 52%|█████▏    | 157/301 [46:40<38:50, 16.18s/it]
 52%|█████▏    | 158/301 [46:57<38:40, 16.23s/it]
 53%|█████▎    | 159/301 [47:12<38:08, 16.12s/it]
 53%|█████▎    | 160/301 [47:29<38:03, 16.19s/it]
 53%|█████▎    | 161/301 [47:46<38:34, 16.53s/it]
 54%|█████▍    | 162/301 [48:02<37:53, 16.36s/it]
 54%|█████▍    | 163/301 [48:17<36:54, 16.05s/it]
 54%|█████▍    | 164/301 [48:33<36:05, 15.81s/it]
 55%|█████▍    | 165/301 [48:49<36:09, 15.95s/it]
 55%|█████▌    | 166/301 [49:06<36:56, 16.42s/it]
 55%|█████▌    | 167/301 [49:24<37:09, 16.64s/it]
 56%|█████▌    | 168/301 [49:39<36:18, 16.38s/it]
 56%|█████▌    | 169/301 [49:57<36:38, 16.65s/it]
 56%|█████▋    | 170/301 [50:13<36:23, 16.67s/it]
 57%|█████▋    | 171/301 [50:30<36:00, 16.62s/it]
 57%|█████▋    | 172/301 [50:47<35:47, 16.65s/it]
 57%|█████▋    | 173/301 [51:03<35:34, 16.67s/it]
 58%|█████▊    | 174/301 [51:19<34:40, 16.38s/it]
 58%|█████▊    | 175/301 [51:36<34:30, 16.43s/it]
 58%|█████▊    | 176/301 [51:52<33:57, 16.30s/it]
 59%|█████▉    | 177/301 [52:09<34:12, 16.55s/it]
 59%|█████▉    | 178/301 [52:26<34:18, 16.73s/it]
 59%|█████▉    | 179/301 [52:42<33:29, 16.47s/it]
 60%|█████▉    | 180/301 [52:57<32:23, 16.06s/it]
 60%|██████    | 181/301 [53:12<31:39, 15.83s/it]
 60%|██████    | 182/301 [53:27<30:35, 15.43s/it]
 61%|██████    | 183/301 [53:40<29:08, 14.82s/it]
 61%|██████    | 184/301 [53:54<28:38, 14.69s/it]tensor(0.2165, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2051, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2025, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2043, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2080, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2058, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2114, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1973, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1985, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1959, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2033, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1962, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1877, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2018, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.2052, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1855, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1945, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1934, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1974, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1869, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1846, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1847, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1954, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1947, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1904, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1893, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1893, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1829, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1821, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1795, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1913, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1800, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1899, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1812, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1855, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1789, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1834, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1871, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1904, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1713, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1767, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1802, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1773, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1813, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1770, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1774, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1737, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1757, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1714, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1694, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1776, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1728, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1799, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1649, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1726, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1761, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1725, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1858, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1646, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1689, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1751, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1699, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1677, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1776, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1770, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1624, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1633, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1705, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1721, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1642, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1720, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1591, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1647, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1633, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1601, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1574, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1595, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1635, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1571, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1539, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1580, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1578, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1509, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1614, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1583, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1541, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1522, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1534, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1577, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1520, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1592, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1533, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1541, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1515, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1537, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1515, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1509, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1525, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1494, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1563, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1560, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1523, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1484, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1450, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1472, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1523, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1432, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss 
 61%|██████▏   | 185/301 [54:08<27:46, 14.36s/it]
 62%|██████▏   | 186/301 [54:21<26:45, 13.96s/it]
 62%|██████▏   | 187/301 [54:35<26:36, 14.00s/it]
 62%|██████▏   | 188/301 [54:49<26:35, 14.12s/it]
 63%|██████▎   | 189/301 [55:03<26:00, 13.94s/it]
 63%|██████▎   | 190/301 [55:17<25:41, 13.89s/it]
 63%|██████▎   | 191/301 [55:31<25:48, 14.08s/it]
 64%|██████▍   | 192/301 [55:47<26:13, 14.44s/it]
 64%|██████▍   | 193/301 [56:02<26:45, 14.86s/it]
 64%|██████▍   | 194/301 [56:18<26:38, 14.94s/it]
 65%|██████▍   | 195/301 [56:32<26:19, 14.90s/it]
 65%|██████▌   | 196/301 [56:48<26:21, 15.06s/it]
 65%|██████▌   | 197/301 [57:02<25:45, 14.86s/it]
 66%|██████▌   | 198/301 [57:16<24:55, 14.52s/it]
 66%|██████▌   | 199/301 [57:30<24:21, 14.32s/it]
 66%|██████▋   | 200/301 [57:44<23:49, 14.15s/it]
 67%|██████▋   | 201/301 [57:57<23:15, 13.95s/it]
 67%|██████▋   | 202/301 [58:11<22:56, 13.91s/it]
 67%|██████▋   | 203/301 [58:25<22:49, 13.97s/it]
 68%|██████▊   | 204/301 [58:39<22:28, 13.90s/it]
 68%|██████▊   | 205/301 [58:52<21:57, 13.73s/it]
 68%|██████▊   | 206/301 [59:05<21:31, 13.60s/it]
 69%|██████▉   | 207/301 [59:18<21:02, 13.43s/it]
 69%|██████▉   | 208/301 [59:32<21:06, 13.62s/it]
 69%|██████▉   | 209/301 [59:46<20:55, 13.64s/it]
 70%|██████▉   | 210/301 [1:00:00<20:56, 13.81s/it]
 70%|███████   | 211/301 [1:00:13<20:23, 13.59s/it]
 70%|███████   | 212/301 [1:00:27<20:19, 13.71s/it]
 71%|███████   | 213/301 [1:00:42<20:24, 13.91s/it]
 71%|███████   | 214/301 [1:00:56<20:10, 13.91s/it]
 71%|███████▏  | 215/301 [1:01:09<19:51, 13.85s/it]
 72%|███████▏  | 216/301 [1:01:23<19:36, 13.84s/it]
 72%|███████▏  | 217/301 [1:01:37<19:28, 13.91s/it]
 72%|███████▏  | 218/301 [1:01:52<19:35, 14.16s/it]
 73%|███████▎  | 219/301 [1:02:09<20:21, 14.89s/it]
 73%|███████▎  | 220/301 [1:02:26<20:59, 15.54s/it]
 73%|███████▎  | 221/301 [1:02:42<21:01, 15.77s/it]
 74%|███████▍  | 222/301 [1:02:59<21:17, 16.17s/it]
 74%|███████▍  | 223/301 [1:03:17<21:38, 16.65s/it]
 74%|███████▍  | 224/301 [1:03:34<21:32, 16.79s/it]
 75%|███████▍  | 225/301 [1:03:51<21:11, 16.74s/it]
 75%|███████▌  | 226/301 [1:04:08<21:12, 16.97s/it]
 75%|███████▌  | 227/301 [1:04:25<20:52, 16.93s/it]
 76%|███████▌  | 228/301 [1:04:41<20:14, 16.64s/it]
 76%|███████▌  | 229/301 [1:04:49<17:01, 14.18s/it]
 76%|███████▋  | 230/301 [1:05:00<15:39, 13.23s/it]
 77%|███████▋  | 231/301 [1:05:14<15:43, 13.48s/it]
 77%|███████▋  | 232/301 [1:05:28<15:37, 13.59s/it]
 77%|███████▋  | 233/301 [1:05:42<15:19, 13.53s/it]
 78%|███████▊  | 234/301 [1:05:56<15:17, 13.70s/it]
 78%|███████▊  | 235/301 [1:06:09<14:54, 13.56s/it]
 78%|███████▊  | 236/301 [1:06:22<14:36, 13.49s/it]
 79%|███████▊  | 237/301 [1:06:36<14:32, 13.63s/it]
 79%|███████▉  | 238/301 [1:06:50<14:18, 13.63s/it]
 79%|███████▉  | 239/301 [1:07:02<13:43, 13.29s/it]
 80%|███████▉  | 240/301 [1:07:16<13:34, 13.36s/it]
 80%|████████  | 241/301 [1:07:30<13:33, 13.56s/it]
 80%|████████  | 242/301 [1:07:43<13:18, 13.54s/it]
 81%|████████  | 243/301 [1:07:57<13:14, 13.71s/it]
 81%|████████  | 244/301 [1:08:11<12:59, 13.67s/it]
 81%|████████▏ | 245/301 [1:08:25<12:57, 13.88s/it]
 82%|████████▏ | 246/301 [1:08:39<12:36, 13.75s/it]
 82%|████████▏ | 247/301 [1:08:53<12:25, 13.81s/it]
 82%|████████▏ | 248/301 [1:09:07<12:16, 13.89s/it]
 83%|████████▎ | 249/301 [1:09:21<11:58, 13.82s/it]
 83%|████████▎ | 250/301 [1:09:34<11:35, 13.63s/it]
 83%|████████▎ | 251/301 [1:09:48<11:34, 13.89s/it]
 84%|████████▎ | 252/301 [1:10:02<11:24, 13.96s/it]
 84%|████████▍ | 253/301 [1:10:16<11:11, 13.99s/it]
 84%|████████▍ | 254/301 [1:10:30<10:45, 13.73s/it]
 85%|████████▍ | 255/301 [1:10:43<10:26, 13.62s/it]
 85%|████████▌ | 256/301 [1:10:57<10:18, 13.75s/it]
 85%|████████▌ | 257/301 [1:11:11<10:10, 13.88s/it]
 86%|████████▌ | 258/301 [1:11:25<10:02, 14.01s/it]
 86%|████████▌ | 259/301 [1:11:40<09:57, 14.22s/it]
 86%|████████▋ | 260/301 [1:11:55<09:55, 14.53s/it]
 87%|████████▋ | 261/301 [1:12:10<09:47, 14.69s/it]
 87%|████████▋ | 262/301 [1:12:25<09:28, 14.58s/it]
 87%|████████▋ | 263/301 [1:12:39<09:04, 14.33s/it]
 88%|████████▊ | 264/301 [1:12:52<08:40, 14.06s/it]
 88%|████████▊ | 265/301 [1:13:06<08:23, 13.97s/it]
 88%|████████▊ | 266/301 [1:13:20<08:07, 13.94s/it]
 89%|████████▊ | 267/301 [1:13:34<07:58, 14.08s/it]
 89%|████████▉ | 268/301 [1:13:48<07:48, 14.18s/it]
 89%|████████▉ | 269/301 [1:13:57<06:35, 12.36s/it]
 90%|████████▉ | 270/301 [1:14:05<05:42, 11.05s/it]
 90%|█████████ | 271/301 [1:14:12<05:02, 10.09s/it]
 90%|█████████ | 272/301 [1:14:34<06:34, 13.61s/it]
 91%|█████████ | 273/301 [1:14:57<07:35, 16.29s/it]
 91%|█████████ | 274/301 [1:15:19<08:08, 18.08s/it]
 91%|█████████▏| 275/301 [1:15:41<08:19, 19.22s/it]
 92%|█████████▏| 276/301 [1:16:02<08:17, 19.90s/it]
 92%|█████████▏| 277/301 [1:16:25<08:13, 20.58s/it]
 92%|█████████▏| 278/301 [1:16:46<08:01, 20.93s/it]
 93%|█████████▎| 279/301 [1:17:07<07:39, 20.88s/it]
 93%|█████████▎| 280/301 [1:17:29<07:24, 21.18s/it]
 93%|█████████▎| 281/301 [1:17:51<07:06, 21.30s/it]
 94%|█████████▎| 282/301 [1:18:12<06:46, 21.37s/it]
 94%|█████████▍| 283/301 [1:18:33<06:24, 21.39s/it]
 94%|█████████▍| 284/301 [1:18:54<06:00, 21.21s/it]
 95%|█████████▍| 285/301 [1:19:16<05:40, 21.26s/it]
 95%|█████████▌| 286/301 [1:19:36<05:15, 21.04s/it]
 95%|█████████▌| 287/301 [1:19:56<04:51, 20.82s/it]
 96%|█████████▌| 288/301 [1:20:16<04:25, 20.45s/it]
 96%|█████████▌| 289/301 [1:20:37<04:06, 20.54s/it]
 96%|█████████▋| 290/301 [1:20:58<03:47, 20.68s/it]
 97%|█████████▋| 291/301 [1:21:19<03:27, 20.74s/it]tensor(0.1408, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1464, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1474, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1558, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1471, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1461, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1439, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1432, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1464, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1490, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1394, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1403, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1437, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1382, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1367, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1378, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1500, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1457, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1430, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1496, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1489, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1500, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1431, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1421, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1449, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1348, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1388, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1351, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1375, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1423, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1371, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1408, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1412, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1463, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1349, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1392, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1386, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1360, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1313, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1383, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1317, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1409, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1466, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1323, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1361, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1384, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1351, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1278, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1363, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1381, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1351, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1329, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1278, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1315, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1395, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1276, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1269, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1391, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1340, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1341, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1374, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1313, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1261, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1345, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1407, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1299, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1246, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1355, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1356, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1400, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1419, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1412, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1314, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1517, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1435, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1391, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1333, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1459, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1286, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1292, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1261, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1247, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1277, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1218, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1269, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1265, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1215, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1205, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1245, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1232, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1266, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1198, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1237, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1185, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1213, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1266, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1240, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1208, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1182, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1200, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1233, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1172, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1196, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1194, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1196, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1120, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1140, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss 
 97%|█████████▋| 292/301 [1:21:37<03:00, 20.08s/it]
 97%|█████████▋| 293/301 [1:21:57<02:40, 20.07s/it]
 98%|█████████▊| 294/301 [1:22:18<02:21, 20.16s/it]
 98%|█████████▊| 295/301 [1:22:38<02:01, 20.27s/it]
 98%|█████████▊| 296/301 [1:22:59<01:42, 20.43s/it]
 99%|█████████▊| 297/301 [1:23:20<01:22, 20.55s/it]
 99%|█████████▉| 298/301 [1:23:40<01:01, 20.56s/it]
 99%|█████████▉| 299/301 [1:24:01<00:41, 20.62s/it]
100%|█████████▉| 300/301 [1:24:22<00:20, 20.60s/it]
100%|██████████| 301/301 [1:24:42<00:00, 20.49s/it]
100%|██████████| 301/301 [1:24:42<00:00, 16.89s/it]
tensor(0.1133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1121, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1105, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1143, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1114, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1153, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1099, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1181, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1125, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
loss tensor(0.1165, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)
Done! Best Results:
{'train_auc': 0.9394353661738754, 'train_f1': 0.9416801602766419, 'train_pos_ratio': 0.9371534195933456, 'train_epoch': 23, 'train_macro_f1': 0.7437616445751665, 'train_micro_f1': 0.8966339138048448, 'val_auc': 0.7702017853821834, 'val_f1': 0.9262865090403337, 'val_pos_ratio': 0.9370860927152318, 'val_epoch': 23, 'val_macro_f1': 0.6561934475703599, 'val_micro_f1': 0.8683774834437086, 'test_auc': 0.7736149929069005, 'test_f1': 0.9210957621166004, 'test_pos_ratio': 0.9304923458833264, 'test_epoch': 23, 'test_macro_f1': 0.6612583606320125, 'test_micro_f1': 0.8605709557302441}
test_auc 0.7736149929069005 test_f1 0.9210957621166004 test_macro_f1 0.6612583606320125 test_micro_f1 0.8605709557302441 